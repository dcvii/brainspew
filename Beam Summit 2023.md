---
title: "Beam Summit 2023"
source: "https://notegpt.io/workspace/detail/GnaKxCVTT2c"
author:
published:
created: 2025-02-27
description:
tags:
  - "clippings"
---
Beam Summit 2023 | Founder's Panel â€“ Robert Bradshaw, Kenneth Knowles, Reuven Lax

![](https://nc-cdn.oss-us-west-1.aliyuncs.com/notegpt/my_notes_images/a8dedcb3ed7f96714b9841ce9a2f33c1.png)

so without any further Ado I don't want to spoil any Geographic introduction I'm going to let the guests present themselves we have first Ken \[Music\] \[Applause\] second ribbon and last but not least Robert \[Applause\] \[Music\] so I I hope I didn't butcher any names I'm Italian I might have an accent I don't know about that so first let's start can you give a brief introduction about yourself let's start from left to right okay oh my gosh what side is that it's my right um yeah so uh yeah I guess I I

have been working you know and like Cloud dataflow for eight and a half years I'm the PMC chair so which means that I write a report to the board of the foundation it doesn't mean I'm other than the day I think do today yes um the rest of PMC and everybody on the deadlift are welcome to contribute please um but yeah so that's you know it's not not actually like a boss position amongst the PMC just like the person who does the paperwork um and yeah before Google I did like analytics for some startups and

not quite non-profits all those benefit Corp the kind of sort of that area um so I was really excited like when I came here I also so I have a PhD in programming languages so I was like really excited to work on something that's kind of like programming languages and addresses all the pain that I had like encountered like in my life trying to build analytics systems where I did like Lambda architecture with whatever rabbit mq and like weird bespoke batch systems it was awful so yeah anyhow veeam is like mwah and I

love open source so okay there we go how about you Robin um yeah so I've been working on beam data flow and predecessors to it for about 15 16 years before before data flow we had a system called Mill wheel so we published a paper on in two I think it was 2010 wasn't it something they're about 2014 I think that was Miller oh no oh sorry yeah that was a beautiful paper um and that was that was an earlier streaming system where a lot of the concepts and being like like Watermark State timers a lot of those Concepts

originated there in millwheel um and then we moved on to build dataflow um and then beam came out of data flow when we took the data flow API and sort of separated it from data flow and open source to this Apache beam before that I was you know working on completely different things I was working on um storage systems at Microsoft for a few years um wow that's definitely a unique part now if my math is correct I'd say that if data flow beam or the predecessor were kids you would have like a teenagers on more or less yeah pretty

much okay and what about you Robert yeah so I've uh um been working on beam again since before beam existed um I started Google working on this thing called Flume that was kind of the predecessor to beam internally actually uh Flume and Mill wheel merged together to make uh beam and data flow so we've got two ancestors here and we took flu we took Flume milwail also a bit of mapreduce and a bit of other systems and kind of like merge them together to create dataflow yeah yeah so uh so I've been

working on that for you know over a decade now and uh before that I was actually a mathematician pure mathematics don't get to use it but I still find it beautiful so okay that's interesting so my understanding is more or less everyone here is coming from at a certain time from Academia and maybe the hat that you had at Google when you joined was software engineer but the reality is that I don't think you were that far away from a researcher at Google or at least when you started it felt like that way

I'd say so how did you research at Google led to like the development of Apache beam like try to bring us in a really short but yeah like as detailed as possible history of what led to the creation of Apache beam I think we should start on the opposite side sure yeah I think um kind of a lot of it is uh finding so so in math what you do is you know you have all these diverse problems and then you find out like what problem could I solve that would like solve all of these I think that's one key carryover that we you know take with

beam is you see you know every everyone has like their unique problem they're trying to do whether they be like you know we got a lot of great talks yesterday you know iot sensors or you know Financial transactions um and being able to boil that down and say okay what is like the really Crux of the issue that we're trying to solve here and if we can make a tool that can do that then you can you know use it in all these various situations and that's something more useful than just kind of solving each one and as a one-off

bespoke system yeah I agree and I think part of the challenge we often faced was this Balancing Act where we're you know we're searching in that sort of math sense for what is the real thing that would be the the most General and most abstract and sometimes you you're not convinced you found it and you you have to do this balancing of that of do we wait indefinitely until we find the one true answer or do we say okay this is what we have um and we're going with this and we may regret it later and there was a lot of playing that

balancing out and I think sometimes we've regretted on both sides sometimes we put things out there that we look back on three years later and we said we wish we had waited we wish I hadn't done that but then there were times that we waited too long and we look back so you know what we wish we had just put something out there and you know it's the benefit of hindsight so I think those those balancing acts are really hard to get right in the moment you're trying to get them right I guess um I came in somewhat later right so I I

didn't research the original systems but sort of more like as being gained its unified model I was involved in sort of the early development of that um and something I'll emphasize which is very it's related um and and it's not just because you primed us with the word research but I think something that we have done differently that has really led to it being the way it is is it's a sort of a uncompromising approach to correctness whereas I think that just a lot of systems especially at high scale

um they sort of they prize getting something that kind of mostly works over sort of solving a problem in a way that's durable and I think that in beam we've really taken the approach of like you're going to have the right answer um and in the event that there's something that's approximate about it that's going to be well described um and that's that's to me what led to like being existing as opposed to you know that's I don't know I don't want to beat up on any past systems that are all very very

useful but like I will I'll say I'll just say like storm right like everybody knew Storm's gonna just like run stuff uh right and it's up to you to figure out how to make it correct um and and with beam we really really wanted \[Music\] correctness to be something that the user did not have to solve like we solved it for you and this is I think this is a little unusual when we first came out with beam um it was people just assumed that if you're running a streaming system you would not get precise answers out of it

I mean how many people here remember the Lambda architecture oh yeah which was a lot of people remember the lab the architecture which was a well-publicized pattern but the whole point of the Lambda architecture was base predicated on this assumption that your streaming system would give you incorrect data so you had to rerun a bat system at the end of the day and use your streaming system to get you know just sort of good enough data so the idea that you that you we could give you a system that would run

streaming one low latency and also give you precise data was strange I mean now it's no longer strains there are other systems that you know you can run Apache Flink as well and get exactly one's data you can run spark streaming and get exactly one's data this was in 2015 or so this was kind of a stranger idea for a lot of people like continuing on This research at Google like how how was it born like a 20 project like uh following up another task was it like no I like this project I'm Gonna Do It full time and I'm gonna

take the risk how did it work how far back are we going to like the mill wheel where you know so you can talk about Miller it was like an actual project so you know Craig Chambers he's you know another researcher he was a professor at University of Washington uh came into Google um did some other stuff and then started you know The Flume projects as he said I have a better way to do mapreduces um and uh but you know there was a time when it was like you know we thought it was going to get canceled

you know and it's like you know the whole idea is just going to get scrapped mapreduce is good enough for anyone um so yeah and you know you when you start something out even if it was you know a full-time project and there was a team of like you know four people you know trying to build the system out trying to find some users trying to just you know validate that this is a useful system and tell people on it um you know it really kind of like you know took a while for the idea to click and then kind of once it took off then

it was more pragmatic and you know then you have all of a sudden have more users than you know what to do with and you're like how do I how do I support everyone how do I make it do everything that's that's being asked and I think that kind of validates it from like being a research idea to being something really useful did you add any chance of getting code review by Jeff Dean or something and the first submissions when you were trying to improve mapreduce no my ID number is two so but Google has this idea of

readability reviews so in every programming language at Google um when you first joined somebody has to us you have to you have to apply for readability review and somebody who's a reader in that language has to prove and say okay you're good in that language so when I first joined Google I applied for readability in several languages one of them was Python and I get a response saying you're a randomly assigned readability reviewer for python is Guido van Rossum and it was an interesting review because

at one point he told me to change something in my code and I went and read our style guide and said I don't get it I don't see anything of the style guide why I shouldn't use this function and the answer was like I don't want you to use that because I want to remove that function of the next version okay fair enough yes uh for uh like Widow and Rossum was the author and creator of python and also worked at Google so I don't know if everyone was aware of this but this is kind of like a really interesting fact

and I wonder like when you came up with the idea or like joined the team at a point where like beam was still like a really kind of like pioneeristic project I'd say like did you expect the project to grow up in such like a nice Community one day sitting in New York and I'll sing a panel about Founders and friends I don't know like we did we had some issues naming the the session but we ended up with Founders I hope you're gonna enjoy yeah um I mean no or I mean not no like this is a dream come true kind of thing right it's like

we we did have we started from like a great place in terms of like open source Community because it was like a a collaboration between someone who you know wrote Flink Runner someone else wrote The Spark Runner um because it came from like the the data flow SDK which had some hooks for like plugging in other engines um and people that we didn't know basically just wrote these things um and and so we we had hopes because we were like okay we're we're putting all these open source projects together dumping

them in a repo getting the foundations like sort of approval uh to start incubating it um but you know I guess I you never you can never be like oh yes this will definitely grow into a like an open source project with like you know hundreds of contributors that has like a conference where we all get together and stuff that seems okay and what about you guys I mean I think yes and no I think we definitely came in hoping that beam would grow and it wouldn't be a completely Niche project but you like like Ken said you

when you put something new out there you never know like we were convinced that this was better than what was out there before and it was an advance over the state of the art but you just never know if the you know if people will jump on it or people be like yeah it's I don't know it's a little complicated I don't quite get it especially the B model wasn't always the simplest model uh to get your head around so I I think I'm definitely happy that it's grown to the point where it's grown today Okay and like I mean when beam was first

released I suppose it was like I mean the model was still there but I suppose it's pretty different from what it is today what like what changed the most and what is like the feature that you contributed to that you're most proud of you like okay y'all I'll take this um so I think uh well what one thing I'm glad of is like the core model has not you know significantly deviated um which is kind of nice to see that like you know what what was what went out there at least you know kind of you know it had legs um but definitely a lot

of uh I think um there's like some of the things that have been added is uh just the portability and cross-language story I think has been huge um you know when like was mentioned you know we started beam we said oh we have like you know a Flink Runner and a spark Runner um but it was kind of like a fake portability story in the sense that we kind of had those um and so I think you know that's that's one of the things I think has really grown is because kind of this idea that you don't have to be locked into a

language you don't have to be locked into a runner and now that um you know since since the founding that's been reified in you know a set of protocols and you know specifications that really allow this this mix and match interoperability what we've seen like in a lot of talks you know this cross language I just want to like grab something that's already written in Java and just use it as is my python pipeline or you know I'm making a new SDK and I can you know just just borrow this whole Suite of things instead of saying oh

well here's like the the nine sources that I have to implement by hand before I have anything useful so I think that that's something really neat has come out one thing I'm proud that we added was the schema supports because the original beam model the the whole concept was like user data is completely opaque it's just every data as a blob users can write a coder or use a built encoder to tell us how to turn their blob into bytes which is just another blob and the schema support added this concept which was not an

innovation it's something that clearly SQL databases have had for decades and decades that you ninety percent of the time there is structure to data data is usually made up of fields and Fields often have specific types and if you expose that structure you can per you can provide ways that are not only easier for users instead of having to write complicated code they can just say you know some buy field Foo like you would do in SQL but also a lot gives a lot more opportunities to optimize things

and so adding that support which we did what about four years ago I'm losing track of time now maybe five years ago it's an ongoing process but that was that's one thing that also changed beam from the original beam model okay I I have a questions for Ken no pressure okay yeah sure yeah what advice would you give to someone who is interested in contributing to Apache beam Do It um well specifically uh don't hesitate I guess um we're very nice and if you whatever reason you think should be in the way of

your contribution it's probably not really like it's it's great you can you know um that's the whole thing right there is a long contribution guide um I'm gonna say like you don't have to read it you don't have to follow what it suggests like if you just make contact and open a pull request that's like the start of our relationship right there okay I I heard stories about like long lasting pull requests oh my gosh if that starts happening just ping me yeah I will step in okay that should not happen

right that's like you have one job as a code reviewer well you have two jobs but like job one be nice be welcoming uh job two make sure the code will not ruin anybody's day later and I'd say sometimes code reviewers the code reviewers lose track or forget that they're reviewing the pr so let's just ping them and say hey you've been give them a friendly ping I said hey do you still remember you're reviewing my PR and half the time they'll probably say oh shoot I forgot about that let me look at that again because you know

everybody's busy right I want to add something which is also a I'm jumping back to a previous question which is related to this which is like if you look at our contributions um in the beginning we thought maybe it'd be a few companies collaborating with big teams each building but what we actually have is we have hundreds of hundreds of people that have contributed once right so like the other thing is like you're in very good company if you have like one thing you just did and you want to contribute it right

that's the vast majority of our contributors okay I I wonder this is like out of curiosity are there any beam contributors who were Bing contributors and then join your team at Google yes yes yeah the answer is yes I was just trying to think of naming names no there are people who've like used me before and then end up joining yeah yeah okay okay and this is the last beam strictly being related question is like and it's a little bit open-ended but what are your hopes and dreams for Apache Beam for the

future of Apache Bean I know I hope it keeps growing like you know it's it's more more use more you know interesting uh cases be you know continues to be helpful to people um we've got I think uh you know schemas was mentioned portable is mentioned there's a lot of projects that are like you know already useful but like have not yet you know reached their full potential um so I think there's you know just you know continues to to grow and flourish and I make people's lives easier okay well personally I'm a big fan of

the running friends part because like that I I find it kind of unique to beam but uh maybe it's me being ignorant so I'm really really happy to be discovering Beam at this time let's put it this way so now let's I mean let's ask questions that are a little bit not strictly beam related beam focused I wanted to insert a question about cryptocurrencies but they're like so 2022 and now in 2023 we have generative AI so now with chat GPT and Bart I'm adding both because I work for Google so I'm biased but uh have you ever tried to

create any kind of like veeam Pipeline and were you able to actually make one that run and work properly so I don't care if it runs it's great for being like what should this code generally look like if you're like I don't really know how to get started boom like you know I don't think I would trust these I you know I'm not gonna have opinions about llms for other things but optimistically it like is great for lowering the barrier to entry yeah I definitely asked um llms to create beam pipelines for me

and yeah they they'll come close sometimes they'll cut you know you'll get things with like stupid mistakes in them that will just never work but are not that far off of working but they like look the way they look the way of these yeah I've also just taken given it spark programs and said hey can you convert the spark program to beam can you prefer this beam program to spark and it is surprisingly good job at giving the equivalent beat version of spark or spark version of B again like sometimes it'll you know

it'll you know it's like put in a less than instead of a greater than or silly mistakes that will make the whole thing not work but if you actually understand what it's outputting it's pretty easy to scan in and be like oh I don't have to fix this and you just saved me 15 minutes of typing boiler play code yeah or it'll just like put two numbers into a tuple and be like there's the hash function like that's a tuple yeah exactly it doesn't matter you know because you it eliminates the blank sleep

yeah I found it it takes a surprisingly good stab at it it you know the the one thing I noticed is kind of like the more you get off the trodden path um kind of the the more it kind of it'll still take a stab at it but kind of like you know a much more blind stab at so you know word cap it could produce his word camel um but uh but I you know they're only going to get better um and uh and so you know what what it can do today is pretty impressive and you know what I could do tomorrow is probably get you know those less than

greater than signs right um more complex pipelines and I I assume the more of the community that grows and more the internet and the training set essentially that's filled with more examples of beam code the better these models will get \[Music\] I wonder like are these where these pipeline all like streaming or also batch and like were there any differences in the quality of the code that the bot was able to do that's the great thing about beep they're both right I was my favorite as I always be

like okay give me a read from a pub sub topic enrich it with from data you read from files something like that right and it looks reasonable you know it looked like word count but but it you know but it had the right things in it yeah you'll get most of the Imports right right but like to anybody who's thinking of doing this if there is a turnkey transform we provide that does the work use that because then when a bug is fixed you'll get the bug fix don't write your whole pipeline this way if it's you know something that can be

shipped okay so uh throughout the conference we handed over books so I do think that as a data practitioner several people here have read several data books personally I read streaming systems was really one of my favorite books so congrats Riven and I have to say like a lot of people usually search inspiration in finding new books and if one randomly goes over O'Reilly the list is quite long would you have any recommendation outside of that book that I personally recommend from beginners to experts I do

think that you're gonna get great value out of it any data book that you would recommend either from O'Reilly or outside of O'Reilly if you don't recommend those and you like fiction that's fine as well but yeah most of my favorite data books are probably the ones from 40 years ago like your books on databases by card and a lot of those original like foundational books and I think at the on the one hand some of that early foundational stuff feels you know a distance from the problems we're solving today but like with a lot

of mathematical stuff you find that a lot of that is still the foundation of the way we think about data and I I always think it's important to have a very foundational understanding of things before you you know build on top of that okay then like if you don't have any data book any interesting read that you would suggest for the audience I'm gonna switch any interesting so far if you like academic papers but yeah yeah I think the papers um on like you know Stream box and all the nyad papers are really inspiring and

really relevant just to name two off top of my head okay then I have an easier question I hope an easier question so when you're not committing or contributing to Apache beam what do you do any hobbies no no I've got this guitar playing I yeah I I play and I'm starting to build guitars I know yeah I have been my myself a fretless guitar which I play every day but um yeah okay I'm in a band we play cool 90s covers in case you would really miss the early 90s okay so we we have a future rock star what about you Robin

um I say traditionally but two biggest Hobbies have been uh swing dancing and rock climbing although the past couple years it's running after a three-year-old definitely more challenging yeah so I'm I'm in the simple I got five kids and that's that's pretty all-encompassing hobby you've got you know uh how do you need scale what oh yeah well I'm parallel processing so I got a twin girls twin boys and then I had a straggler you know but uh but yeah they kiss you busy I got you know they you know we like camping

my boys in robotics Club um just you know stuff like that so okay then the last question and then we're gonna switch to audience question feel free to ask the most embarrassing question you come to mind that's not a problem that's gonna be their problem but uh the last question is like for the future if you could decide the next location of the beam Summit assuming that we need to have like a Google facility nearby for Budget reasons very likely but where would you be like where would you like to see the next beam

Summit thank you clearly New York um I would love to go back to London we did some events in London before the pandemic it would be nice to go back there I always feel conflicted because like if you if you do it in Hawaii I'm not going to come to the summit talks we have an office in their wife no no okay that was news for me it was a dream you have a year to build the Google office in Hawaii you only you just need one desk and it's a Google office okay what about you Robert yeah I mean I I think it would be nice you know like

to alternate like Europe because you get you can get a different crowd because um you know they're they're definitely you know people here from all over the world but uh um it's a lot easier to go to Osama that's like you know a two-hour flight away then you know 10 or 20 hour flight away okay so thank you again any question from the audience uh well since guys you're all already here uh uh what do you think about beam 3.0 what do you think it should be there the what zero you know I guess like the question of

like how to actually evolve beam in a way where we have to make changes that would cause someone to have to like you know adjust the pipeline that's been running for like five years it's it's tough to make those kinds of calls um yeah there's there was an icon 3.0 I do have opinion I think like schemas should be the default everywhere right and like we have we've I've seen two different mentions in talks at this Summit about like the default like just use the pickle coder or the Java serializable code right like that was a

mistake but now we have data encoded that way so we don't have a choice of just like switching that without breaking a bunch of people um yeah we do have years of sort of experience of all the crusts and everything that was wrong with me you know and things that we can't really change completely because you know it's part of me um so yeah making schemes of default say everything is a schema maybe the windowing triggering API which has proved to be very confusing to a lot of users we had a lot of ideas of here's a

better way of doing triggering and it was never done partially because no one ever had time to do it but partially because it would if we can't get rid of the old triggering API so now you would just have yet another triggering API that we'd say this one is better but all the examples on the internet use this old one yeah um so there are a lot of things like that where you have the opportunity to clean things up make things you know simpler more understandable more expressive without having to worry about

about you know compatibility with the old thing um practically I mean I don't know yeah I think being 3.0 is something in the card soon or do you think it's I think it's also hard because if you do a radical change then you're going to live with the two being 2.0 and being 3.0 simultaneously and this is almost a worse position to be in um so you know I think there's there's you know beam 1.0 to 2.0 is because or I guess it was really 0.x to 2.0 yeah um was you know we had like this nested system and then we've kind of like

finally figured out what we're really gonna do um but uh we you know breaking change is different when you have you know thousands of users then you have tons of users and I think it's a different Viewpoint sometimes there's the pure OSS Viewpoint and there's a support you Viewpoint so from the pure beam community you know we can the bean Community can decide how much you support the users who come and ask questions about the old version of beam versus um you know how much you say just rewrite your code to the new version of

B we also wear another hat in that we also work for Google and the dataflow team so we have customers of dataflow who come for support and are paying customers and then you're in a harder position to just say hey go rewrite you you have 50 000 lines of code just please rewrite it all because we don't want to support this other thing starting tomorrow people aren't very happy to hear that but maybe if you know Chad GPD or Bard is good enough yeah we just provided LOM to you buggly transfer yeah here's a

translation utility there are large open source projects where their Community has decided they value improvements over compatibility I don't think this is that kind of compute like Community really but like that's a that's a decision that could be made but I think the question when you mention me through I know there's like what should be in it like this is just like oh yeah what are your dreams for things that you could never do um but then there's also like how do we try to bring that you know bring that to

people right and like for starters building it all behind a flag is not you know we could do that and then you're still living with beam 2.0 and 3.0 at the same time but you can just like you pass the beam three flag and then it does the things like choose good coders or enable you know disable the bad triggers or even enable the good triggers that kind of stuff you stick that behind the flag and eventually like that's what you release yeah actually yeah actually my question was caused by this because

uh we have a lot of deprecated and Legacy stuff that sometimes we drop it sometimes we just forget about it forever so uh it causes of course breaking changes so and this is why I actually usually the next major version of their product it's for that so you start to work on this uh in Peril of course because it's not easy just switch to real next major version just as the next release but you start and work on this and you bring like a only the best thing or keep only the things that up to date in this branch of course

it's double work uh for us as a developers but I'm a little bit wondering because well since I'm quite along on the project but we really talking about this I mean that we're talking about some Legacy stuff some annotations it's uh a little bit useless for now but we are not talking about what we will do kind of to overcome this and maybe the next major version is a good thing for that that was a reason for my question yeah yeah I think there is there is a lot of deprecated stuff you know we're gonna

get rid of reshuffle right someday maybe well really we're just going to rename it and then rename will be deprecated but uh um we already did it's called yeah it's redistributed I think it's like in the repo um but I think a lot of a lot of what we can do we can also do we found you know kind of evolutionarily um and I think it might be an interesting idea to you know take some of these deprecated things that we haven't wanted to remove and make it so that they throw an error if you don't you know pass a flag that says use

deprecated stuff um and then after a while doing that we can actually get rid of it and clean things up um but you know at a slower Cadence then you have to like you know decide am I living in the old world or the New World um more like you know how can I get from the old world to New World more of a Continuum um but and there are things like we just removed the jrh in January um where you know portability um where we're slowly moving to that world and getting rid of the old world at the same time because if you don't

get rid of the old things you're just adding work for yourself I want to just add context for the audience that this is Alexi he's on the PMC asking other members so this is a decision to make with us okay any other question uh \[Music\] so if I'm correct then first kubernetes was open source then tensorflow was open sourced and then beam came to life a year later so I think it was 14 15 16. has it been easier to open source the data flow SDK than the previous projects because it seems that there was a lot of

discussions according to um yeah sources that when kubernetes was open source that there was a discussion of do we really want to give this out and when I first saw the data for SDK when it wasn't Apache beam I was like that's super clever all this stuff is done um so um were there similar discussions when uh beam was open source or has it been this move um yeah right to to start this open source project do you mean internal to Google yeah I don't think I don't remember that there was much so the dataflow SDK was

open source from the beginning even before beam so I don't I don't think there was any pushback on open sourcing that but at the time it was originally there was as Robert said that we didn't have any other Runners and then we kind of had a fake uh spark Runner so it wasn't originally something that was very useful without dataflow right it wasn't until later with Bean that other writers were created that it became an actual useful thing it was originally just you still have to use Google system but here's an open source

SDK for how to use it right well it wasn't it's not like you could have contributed to it meaningfully when it was the dataflow SDK it was just like we built it internally we threw it over the wall and it was you know largely a way of saying like we're gonna run this code on your VMS on your data and like you can know what it is I think the fact that it was like a fat client that we had to give to users anyway yeah made it so that you know when you said open sourcing it's like there wasn't anything that we were

releasing I mean you there was you know we were kind of relinquishing control by starting an Apache you know separate project but it wasn't like we had this you know code base that we said do we want to expose it so that um from a technical standpoint made it a lot easier and you know then like you know once we wanted to do it there was an argument about like you know are we really getting trade secrets well it's already out there we're just you know making it more clear that this is going to be an open source Community

not just you know a pile of code on GitHub there was some process making sure that we didn't accidentally release some piece of Google code that was considered to be trade secret uh not just with the SDK that we released we also released um binaries that we would put on the dataflow VMS and we had to make sure when we built those binaries we didn't Link in any part of Google code that was like a secret of how Google search works or something which is surprisingly easy to do because Google has a big mono repo

that everything's linked into so there was process of this I think like fingerprint 11 or whatever um was this you know big thing where they didn't want to release it and then it it turned out someone had accidentally leaked the code like a year before as that whole thing became null and void so that's that was a hash function that was is commonly used at Google and that was yeah that was an escalation because the security team was afraid that releasing Google's a commonly used hash function would be a

security Vector against uh for against Google and then that ended up being fine but yeah there were a few escalations but for the most part it wasn't too bad there weren't any philosophical objections everyone was pretty on board with this this is the right thing to do let's let's give it to the community and there were discussions of at the time of open sourcing all the internal to dataflow as well and we finally decided that that just didn't make as much sense for a number of reasons but you know the internals of

dataflow are very very specifically authored to Google's Tech sac and running on Google's infrastructure so a lot of it wouldn't even be as useful to the general public as it is to us yeah okay oh so you mentioned that it's probably a schema on pressing schema is better for the future since the measure of the data is as actually as a structure so I'm curious to hear about the history context behind why it was like came a less in the first place to talk you're talking about the internal history I feel like why we

started with just um just like binary blobs as elements yeah I and I think some of that is because that was the history of it came out the history of some of our systems at Google mapreduce and Flume both just said elements are blobs and user rights code to interpret those elements well internally everything's a Proto yeah so it was it was structured we weren't exposing that yeah that's true yeah yeah at Google 95 of the time it's your blob is really a protocol buffer that has the head structure right

um and there were systems so Dremel which was an internal version of what became bigquery of course did understand produce and let people write SQL Expressions over them but historically mapreduce and Flume just you know took this philosophy that it's a system for user writing custom code so the user can just write the code to access their Proto I think I mean I think there's also just an aspect of like we didn't know what users data was going to look like because we invented like beam has its own concept of row with its

own encoding for better or worse and like going out the gates with something like we've got our own row with its own encoding I feel like that's a bit presumptuous almost like when we were just moving and one of the things we did with the schema support and beam is we tried to not move too far away from that so you can take your object you know if you have a pojo or a you know or a Java Bean I'm talking about the Java API right now or an Avro message and keep it as an Avro message and keep it as a keep it or keep

it as a Java Bean but still have a schema attached to it and beam is very smart about knowing that you know oh you want to access field username that's this getter on the Jaffa beat so we don't force users to convert everything into our row object to just to use schemas right that's that's the actual the actual answer is that it can it started as like you're a Java programmer you just want your Java objects to like flow through your pipeline and not even really know their data that's that's actually the origin story of that yeah

frankly there's no Java standard for schema data there's you know a dozen of them that that all need different needs and fall in different spaces so yes second row so I guess this is a it's kind of sideways to beam 3.0 which is more of what are things that you you had done for beam 2.0 what are the the things that are like pinpoints or decisions that you know in retrospect didn't turn out as well as planned I could I mean I could name a list triggers I think were too complicated side inputs I would probably rethink how

side inputs work triggered side inputs the worst thing yeah I wish schemas had been part of being 2.0 from the start instead of like put you know putting it on top because you know that least implementation difficulties we kind of had to layer it on top of coders um and also it means that for all that we think schemas are a much simpler more understandable way of writing code the average user of so many examples on the internet just show the old way of parse your message using a coder that people

are sometimes don't figure out that schemas are there okay do we have any other question um okay so I'm curious uh because beam has so many different Runners like uh flake Runner spot Runner so how do you ensure that the beam is uh going with the same Pace with other other Runners so that when they upgrade their versions that we can keep up with that I mean I guess the short answer is that we don't um sorry I'm too blunt um I think each Runner has like it will keep up with sort of proportional to how much

interest people have in the runner um the we do try not to make like wild changes that would require a nurse to like Implement huge amounts of stuff um that's what I mean that's kind of why like a number Runners still like more or less work right because we have we've actually had even more than like there's you just named like the the big ones that um but there's a whole bunch more that people have written right um yeah anyhow that's I guess the yeah the real thing is like I I think about this

a lot is in the in the open source you're gonna have people come in for like a little while and then maybe they they go to something else or maybe the different companies and team sizes and usage Ebbs and flows with different Runners um and we just have to I mean I'll leave out technical details but but generally just like allow things to be in whatever state they're in and not consider them to be like broken if they have a certain set of features then that's the features they support and always try to you know catch be able to

analyze what the user is doing and say like oh yeah I can run that or I can't run that and just communicate clearly what the capabilities are and the cool thing is each Runner kind of has its community and you know the community is responsible the people who use it are responsible for updating it and the best position to see if it works and then also uh with the portability story we we take the beam pipeline we break it down as two as Primitives and as long as the runner supports those Primitives which are relatively slowly

changing compared to the rest of the things um it should just work um so it's kind of like you know saying well you know you have the jvm and you know the language continues to evolve but as long as you compile the byte code um you don't have to worry about it working and beam has gone ahead of Runners before so one example was um an ordered State object was added to beat where you can store sort elements and beam would keep them ordered and the Flink Runner did not have an efficient way of implementing it

and for a while there was a port some Flint contributors spent some time looking at ways of adding it to flank I think I think this never actually happened in flank so the flank implementation is probably still some inefficient thing where it just stores it in a less than the sourceless when you read it but it did create for a while like there was some interest and there was some discussion on the Flint Community about changing Flink itself so it would have the ability to store um order state okay other questions

uh I see two one up there and one here \[Music\] hello yeah you you mentioned Community several times you mentioned that I can maybe initially thought there might be like several big players coming and that and they'd have has like there are a lot of small contributors contribute ones then what's your goal for growing the community and what do you think has the ideal way the best structure of the community hmm gosh I feel like that question was at me um yeah I don't know I the truth is I I think that

um you know more interested contributors of I have a different perspective now than I did because I think um companies I think users like the best open source Community are our users um right and having big players all investing and building it and they're all selling it for example right I I don't actually think that model's as good as having like power users like building the stuff that they really need and and sort of like they they know how to use this project and they know how to contribute to it

um because honestly they're like more invested in it I feel like companies can be fickle sometimes but anyhow I I shouldn't say too much I any Community is great um but my favorite thing is when users convert to contributors I think there's the community can take out many different shapes but kind of one of the important aspects for me is sustainability and you know it doesn't matter you know who's contributing this long as there are contributors there there are people willing to you know say this doesn't work for me or this works

but it could be better and you know just say okay I'm just going to contribute back um you know if you've written beam code you know 80 of the library is beam code and so all beam developers could just you know say well this worked for me this was useful for me you know I think it'll be useful for someone else and then you know it could be you know people who are employed it could be people who are doing it for a hobby lots of times it's just people who you know they they need to use beam for something and they see

something that could be improved and they improve it and uh as long as you have a critical mass of those I think that's key I want to on that note just sorry for jumping in um you know best what you need kind of like one of the things about that I really love about having a million one-time contributors is that tells me that beam is more effectively meeting users needs than it possibly could if it weren't open source right because we have like having people at a big player think they know what you want and then build it uh

is not even close to as good as if you know what you want and you build it right I mean obviously you have to do the work then but but in general like the the project will be much better suited for for what you're trying to get done if it has like user contributions which was the other question I I think I remember that there was somebody in the last Lane yes okay he just wave his hand personal color with white T-shirt say third seat from the right here no I cannot see you sorry somebody can you raise your hand if you still

want to ask the question no nobody wants to okay so if we don't have any more questions I'm gonna just ask one last one from me but out of curiosity you work with customers you've seen like internal versions of Apache beam and like how it is used at Google how what is the longest running pipeline that we've seen maybe running right now and like it's I mean there's streams exactly right yeah run for years sometimes run for years and It's tricky because people can up we support this update where you

can update a streaming pipeline to the new one so you definitely find pipelines that you know this one pilot has been running for a month but it was just an update of a previous python which is an update of the previous pipeline which going back for multiple years including the updates so the original John job was like Spin and then updated updated updated but traced back up to that time yeah so we've also seen pipelines running I mean persons in pipelines have been running for over a year without an

update okay um and we've seen escalations from customers something went wrong with the pipeline and it starts off this has been running for 18 months why is it breaking now and our reaction is like cool it ran for 18 months \[Laughter\] yeah or I mean surprisingly there are batch pipelines like run for a week like we had you know this with uh with internally at Google we you know it creates a lot of temporary files and we we have the system where our file system will automatically delete after you know

three days or five days you know someone said their TTL to be like don't delete this file for like 14 days and their batch job will learn for so long that like all of a sudden the temporary files are deleted out from under them and you're like that's a big job all right so yeah but that's some very there are some very long-lived pipelines out there um it's and occasionally that's a problem because they're running an extremely old version of Apache a beam an extremely old version of the code and

when there's a problem like you have to do archeology to figure out what version of this code from two years ago is being run on this pipeline yeah I'm familiar with customers having these problems unfortunately but yes \[Music\] okay and a round of applause for the panelist I think

### Summary

The Beam Summit 2023 Founderâ€™s Panel featured discussions on the development and community growth of Apache Beam, emphasizing its evolution and contributions from users.

### Highlights

- ðŸŽ¤ **Panel Introduction**: Founders Ken, Reuven, and Robert shared their backgrounds in Beam.
- ðŸ“ˆ **Community Growth**: Apache Beam thrives on diverse user contributions, fostering a vibrant community.
- ðŸ”§ **Technical Evolution**: Beamâ€™s model has maintained core principles while evolving features like schemas and portability.
- ðŸ“š **Open Source Philosophy**: Emphasis on user-driven development leads to better solutions than top-down approaches.
- ðŸš€ **Future Aspirations**: Panelists expressed hopes for continued growth and enhancement of Beamâ€™s capabilities.
- ðŸ¤ **Contribution Encouragement**: Encouraged new contributors to engage, highlighting a welcoming environment.
- ðŸ¤” **Long-Running Pipelines**: Discussion on the longevity of pipelines, some running for over a year without updates.

### Key Insights

- ðŸŒ **Diverse Contributions**: Apache Beamâ€™s strength lies in its broad base of contributors, ensuring the project meets various user needs effectively. This user-centric approach enhances the systemâ€™s utility and relevance.
- ðŸ”„ **Continuous Improvement**: The evolution of Beam, particularly in schema support and cross-language capabilities, showcases the projectâ€™s adaptability to user feedback and technological advancements, positioning it as a robust tool for data processing.
- ðŸ“Š **Correctness Over Compromise**: The panel emphasized the importance of correctness in data processing, advocating for solutions that ensure precise outputs rather than settling for â€œgood enoughâ€ results, setting Beam apart from traditional systems.
- ðŸ› ï¸ **Challenges of Legacy Systems**: The discussion highlighted the difficulties of managing outdated systems while striving for progress, underlining the balance needed between maintaining user satisfaction and implementing new features.
- ðŸ§© **Interoperability**: Beamâ€™s design allows integrations with various runners and systems, fostering a collaborative ecosystem that benefits users across different platforms and languages.
- ðŸ” **Research Influence**: The foundersâ€™ academic backgrounds significantly shaped Beamâ€™s development, emphasizing a research-driven approach to solving practical problems in data processing.
- ðŸŒ± **Sustainable Community**: The importance of nurturing a sustainable open-source community was underscored, with a focus on user involvement and the transition from users to contributors to maintain momentum and innovation.

I'm your AI assistant. Feel free to ask me anything about this note!

Experience the magic of NoteGPT AI Chat right now!

## Beam Summit 2023 | Founder's Panel â€“ Robert Bradshaw, Kenneth Knowles, Reuven Lax