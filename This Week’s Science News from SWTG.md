---
title: "This Week’s Science News from SWTG"
source: "https://sciencewtg.substack.com/p/this-weeks-science-news-from-swtg-555?utm_source=post-email-title&publication_id=1593492&post_id=157642821&utm_campaign=email-post-title&isFreemail=false&r=7br8e&triedRedirect=true&utm_medium=email"
author:
  - "[[Marcus]]"
published: 2025-02-21
created: 2025-02-21
description: "Nuclear Fusion & A Quantum Breakthrough?"
tags:
  - "T50"
---
![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05d72450-1b19-4b0a-8b41-9ca1fc6e736f_2000x1125.png)

Helion uses an approach to fusion called field reversed configuration that, while not new, is only pursued by three startups: Helion, General Atomics, and TAE Technologies.

These field reversed devices are basically particle accelerators. You accelerate a bunch of fuel particles with magnets and then collide and compress them in the middle. That compression increases pressure and temperature and fuses the nuclei. The energy released in the fusion creates very fast-moving particles. One then has to find some way to harvest that energy.

To give you a sense of scale, Nuclear fusion reactions typically take place in the range of 10 to 100 megaelectronvolts of energy per particle, about a million times less than the energy that the LHC uses, which is why the Helion device is just a few meters long.

What’s special about the Helion approach is that they want to generate electricity directly from fast moving charged particles in the plasma.

You see, in approaches using magnetic confinement like ITER, one generates heat and the heat then has to be used to create electricity. This is also the case for inertial confinement, where one shoots at fuel pellets either with laser particle beams or solid bullets. This detour from nuclear energy to heat to electricity is far less efficient than going from the fast moving particles directly to electricity.

Helion’s is a pulsed approach, not a continuous one, and the fast-moving charged particles rapidly change the magnetic field which induces a current. So that is the advantage.

The disadvantage is that the fast-moving particles need to be charged. Now, in the usual nuclear fusion reaction, one uses deuterium and tritium as fuel. This releases fast-moving neutrons. Yes, one also has charged particles of course because the electric charges need to go there. But the fastest moving particle, that is neutral. So that doesn’t help them much.

This is why Helion, as the name suggests, wants to fuse Helium-3 with deuterium. Because this releases a fast-moving proton, which is charged and they can extract the energy from that.

Sounds good, but there is a reason why no one else uses Helium-3 as a fuel for nuclear fusion. It’s because the energy threshold is [about four times higher](https://www.energyencyclopedia.com/en/nuclear-fusion/thermonuclear-fusion/fusion-fuel).

With that in mind have a look at [this 2021 press release from the Helion website](https://www.helionenergy.com/articles/helion-energy-achieves-100-million-degrees-celsius-fusion-fuel-temperature-and-confirms-16-month-continuous-operation-of-its-fusion-generator-prototype/).

> “Helion Energy \[…\] today became the first private company to announce exceeding 100 million degrees Celsius in their 6th fusion generator prototype, Trenta. Reaching this temperature is a critical engineering milestone as it is considered the ideal fuel temperature at which a commercial power plant would need to operate.”

Note how carefully they write that “it is considered the ideal fuel temperature” and not that they consider it to be ideal. That’s because it’s considered a pretty good temperature for a reactor that runs on deuterium and tritium. For the Helium fusion which they need, that is pretty low.

There is also the question of where the Helium-3 comes from, because that particular isotope is very rare on Earth. Helion wants to create the Helium-3 from deuterium collisions. And while that is possible, it will reduce the efficiency of the entire cycle. Finally there is the general issue that the nuclear plasma motion can become unstable which will lead to losses and also make net energy generation more difficult.

Generally, the company has been very secretive. They have, for all I can tell, not published any technical reports, so really, we have no idea what is going on. I have little doubt that they can generate nuclear fusion reactions in their devices. But I strongly doubt that they are anywhere near breakeven.

That leaves the questions: Why is this company in particular so extremely well-funded? Do the investors know something that we don’t know? Is it reengineered alien technology? Are they secretly building a portal to a parallel universe? Or is it maybe wishful thinking? Let me know which one of those you think is most plausible.

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc2a608c6-0cc1-4b47-96fc-17a6f6828aea_1280x853.jpeg)

*Image: Gage Skidmore via Wikimedia Commons*

For two months, I’ve been working on a video with the working title “Should we defund academia?” Well, reality is moving faster than me. Thanks to Donald Trump and Elon Musk, I might have to rename my video to “Was it a good idea to defund academia?”. So many things have happened in the past two weeks. Here is my 5 minute summary – and a few comments.

Trump’s actions are not directly aimed at academia. Instead, he is targeting initiatives to improve diversity, equity and inclusion, DEI for short, which Elon Musk has referred to as a “woke mind virus” and which have become widespread under the Biden administration. Americans have used this term to broadly describe all actions that aim to increase the share of currently underrepresented groups in certain professions, which can lead to less qualified people getting hired or funded.

Directly after his inauguration, Trump issued an executive order titled “[Ending Radical And Wasteful Government DEI Programs And Preferencing](https://www.whitehouse.gov/presidential-actions/2025/01/ending-radical-and-wasteful-government-dei-programs-and-preferencing/)” which says that “to the maximum extent allowed by law,” each federal agency must eliminate all federal DEI and “environmental justice” offices, positions, actions, initiatives, programs, grants, contracts, and performance requirements.

On January 21, 2025, he followed up with an order aimed at private business titled “[Ending Illegal Discrimination And Restoring Merit-Based Opportunity](https://www.whitehouse.gov/presidential-actions/2025/01/ending-illegal-discrimination-and-restoring-merit-based-opportunity/)”. It encourages “the Private Sector to End Illegal DEI Discrimination and Preferences” and says that DEI is “dangerous” and “can violate the civil-rights laws of this Nation.”

Briefly after this, agencies such as the Center for Disease Control, CDC, and the National Science Foundation, NSF, were removing datasets from their websites. According to AP, more than 8,000 webpages and 3,000 datasets disappeared, mostly concerning public health and environmental studies.

The medical advocacy group Doctors for America quickly sued, claiming that the removal prevented doctors from effectively fighting disease. [Last week, a Washington judge issued a temporary restraining order and said the agencies need to restore all relevant information](https://www.reuters.com/legal/trump-administration-ordered-restore-us-health-agency-websites-that-were-2025-02-11/).

Next, [on January 26, Trump ordered the shutdown of USAID](https://www.state.gov/implementing-the-presidents-executive-order-on-reevaluating-and-realigning-united-states-foreign-aid/), an agency with 10,000 or so employees which mostly funds food and humanitarian assistance across the world. The order says, quote “the United States is no longer going to blindly dole out money with no return for the American people” end quote. The same day, workers pulled down the agency’s letters from the Washington building.

USAID, however, also finances research at American universities. Those who had been awarded grants made financial commitments and were understandably distressed that the money might not come in. [Employees of the agency weren’t amused either and immediately sued the Trump administration](https://www.reuters.com/world/us/trump-administration-sued-by-government-workers-over-slashing-usaid-2025-02-07/). A day later, [a federal judge in Washington](https://storage.courtlistener.com/recap/gov.uscourts.dcd.277213/gov.uscourts.dcd.277213.15.0_2.pdf) ordered a temporary halt to the USAID shutdown proceedings.

[Meanwhile, in response to Trump’s orders, on January 27 the Office of Management and Budget](https://www.pillsburylaw.com/a/web/wUiQFVxqrXj5PKUPv9nY8Z/omb-memorandum.pdf) temporarily paused all academic grants to allow federal agencies to identify all “activities that may be implicated by \[Trump’s previous\] executive orders, including, but not limited to, financial assistance for foreign aid, nongovernmental organizations, DEI, woke gender ideology, and the green new deal.”

Next, Trump came for academic overhead costs. These are the expenses that are added on top of a research grant and that finance administrative services that the hosting institution provides. That’s everything from office maintenance to IT support. In the US it ranges between 30% and 70% of a grant’s total value.

The Trump administration capped the overhead at 15% which would save an estimated $4 billion. This decision, too, was quickly blocked by a federal judge on the grounds that it’d cut into the very infrastructure that supports groundbreaking research.

Personally, I think the major problem here isn’t the goal of the Trump administration, but their means to reach that goal. I am afraid that the current procedure is lumping legitimate and nonsense research together indiscriminately. And while I have myself criticized research overheads that are almost certainly the major contributor to university admin bloat — and many private funders do put a 15% cut on overhead, for example the Templeton foundation — this isn’t a change that an institution can implement on the snap of a finger. Never in the history of universities have they implemented anything on the snap of a finger. At this rate I’ll soon have to rename my video to “Remember When We Had Academia?”

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b14a912-2c1f-4e13-8cdd-63c33f75aaa3_1280x834.jpeg)

*Image: CERN*

The incoming CERN director recently said that the LHC upgrade could show us how the universe will end. That’s quite an upgrade because so far, it’s merely been showing us how particle physics will end. But let’s have a look at what this is all about.

CERN, host of the currently largest particle collider in the world, the Large Hadron Collider LHC. They will get a new director next year, a British experimentalist by the name of Mark Thomson.

[In an interview with the Guardian, he said that](https://www.theguardian.com/science/2025/feb/03/ai-to-revolutionise-fundamental-physics-and-could-show-how-universe-will-end):

> “There’s a particular measurement about the Higgs boson that is so fundamental to the nature of the universe” which is when the proton collisions produce “not one Higgs boson but two Higgs bosons at the same time.” The Guardian explains that this “could reveal whether the Higgs field has reached a final, stable resting state or whether a drastic transition could occur in future, a scenario that would see the universe as we know it evaporate almost instantaneously.”

What he’s talking about is the stability of the vacuum, the empty space underneath all the particles that matter is made of. We usually think of the vacuum as nothing, but particle physics has taught us otherwise. What we call the vacuum is really filled with the Higgs field.

The Higgs field should not be confused with the Higgs-boson. The Higgs field is an even condensate that forms in the early universe and that gives masses to particles by dragging on them. The Higgs-boson, on the other hand, is a ripple on top of the condensate, metaphorically speaking. The reason we usually talk about the Higgs-boson is that it’s the only way we know how to confirm the presence of the field. This is why the Higgs boson discovery was such a big deal.

The Higgs boson has a particular property known as self-coupling, which is basically what the name says: the strength by which it interacts with itself. It’s a parameter in the standard model that isn’t very well known because the best way to infer it is from proton collisions that produce two Higgses as Thomson said.

[You see examples of this in these diagrams](https://cds.cern.ch/record/2643544/files/diagrams.png). Going in left are two gluons which come from the protons and are marked with a g, then there’s stuff going on in the middle with top quarks, and out come two higgs bosons which interact with each other.

It's been somewhat controversial whether the LHC would be able to measure this coupling, or whether the signal would just drown in noise. I am guessing this is why Thomson is stressing in the Guardian article that AI is going to help and that “Now I’m confident we are going to make a good measurement.”

This Higgs-boson self-coupling now is one of the parameters that determine the stability of the vacuum. By the presently possible estimates, it looks like the vacuum around us is very long lived, but not totally stable. It is a meta-stable state, as physicists say.

This is much like if you shine light on a phosphorescent material — it will kick an electron into an energy level where it can remain for some time. But ultimately that state isn’t stable, so the electron falls back down into a lower energy level. That emits a photon which is why these materials have a long afterglow.

If our vacuum is also meta-stable, it will eventually decay into a state of lower energy. But the excess energy that is released in that process will not just create a nice little glow, it will tear everything apart.

But current estimates say that the half-life of the vacuum is extremely long, about 10^500 years. By that time all stars would have burnt out and collapsed to black holes and even the black holes would have evaporated. However, this being a quantum process there is a small probability that it happens any day now. But don’t worry, we’d all be dead before we even knew it was happening.

I find Thomson's statements interesting for two reasons. One is that this measurement of the Higgs self-coupling is the biggest selling point for their next bigger particle collider. So now he is basically saying they don’t need it.

It’s their biggest selling point if you leave aside the ones that they’ve have made up, like that the next bigger collider would tell us something about dark matter or dark energy and other nonsense.

The other reason it’s interesting is that Thomson seems to agree with me that the next bigger particle collider will not discover any new particles. This is because to believe that the Higgs self-coupling tells you something about the stability of the vacuum, you need to assume that there are no other particles to be found. If there were any, these would also enter the calculation. It’s somewhat surprising in that I don’t know any other particle physicist who actually believes that.

This makes me suspect it’s not what he meant to say. I suspect he either doesn’t know how the calculation works or he was sure that no one who knows how it works would mention this little problem. But that’s what you have me for.

In summary: Particle physicists are trying to measure how the Higgs-boson couples with itself. Regardless of what they measure, this will not tell us how the universe will end. And they still haven’t learned that each time they lie to you, I will be here to call them out on it.

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71d71344-6eb3-4168-9800-644d23cd3851_2348x1762.png)

*Image: Liu et al, Sci Adv 11, 5 (2025)*

The way people think I make videos is that I wake up in the morning, look up some cool science news, talk about it for 5 minutes, done. The way it actually works is that I sort through a thousand crappy press releases and then despair over something like “Quantum mechanics experiment measures a pulse of light in 37 dimensions”. I have no idea what that means.

But after thinking about it for a week and several existential crises, I believe I’ve figured it out, so here is my 5-minute summary.

The authors of [this new paper](https://www.science.org/doi/10.1126/sciadv.abd8080) have found theoretical and experimental proof that quantum mechanics is weirder than we thought. Not just verbally, but in a quantifiable way that could be used on quantum computers. It’s actually pretty cool.

And it gives me an opportunity to explain how superdeterminism works.

In the new paper, they study a variant of a problem known as the GHZ paradox. It’s nothing to do with gigahertz but stands for Greenberger-Horne-Zeilinger. For this you need three quantum particles that have at least two different properties. Typically, one uses electrons and measures their spin in two different directions, say horizontally and vertically.

An electron is really just a tiny spinning ball except it’s not a ball and it’s not spinning which is hard to draw. So I hope you’ll excuse that I’ll use coins for illustration instead.

These coins have two properties that correspond to the electron spin in two different directions. Let’s say they’re either red or blue, and they have plus 1 on one side and minus 1 on the other. We will measure either the colour or what side is up. In analogy with the electron spins, we say that if the coin is red, we will count this as plus 1 and if it’s blue, we will count this as minus 1. If the coin lands on its edge, we’ll just pretend it didn’t happen.

For this quantum paradox, one needs three of these coins. We throw them on the table, and then we make measurements on them. A possible sequence of measurements is that we measure the colour of the first coin, and the sides of the two other coins. For each of these measurements we get a result that is either plus one or minus one. And then we multiply them. Here is an example for the first set of measurements. Let’s say the result is plus 1.

We then make two further measurements in different orders, and let’s say the result is also plus 1. This doesn’t have to be the case, but it’s a possible outcome and it just makes things easier to look at this particular example.

You can see that in this sequence all the side-measurements appear twice.

This means that if we multiply all the measurement results, the side-measurements will square to 1, regardless of whether the +1 or -1 sides were up.

So from the previous three results we always know that multiplying all three colour results will also give 1, and we can also see this in our table.

The problem is that if you actually make this experiment with electrons and their spin, the result is minus one. This is the paradox. There is no combination of coin results that can give the result that we actually observe. So what’s going wrong?

The problem with our calculation is that we assumed that the coins have values that are either +1 or -1, and that these values do not change if we make further measurements.

Take for example the second coin. What we measured here is first which side is up, then the colour, then again which side is up. This means we assume that the side doesn’t change if we measure the colour.

But electrons are quantum particles. And for quantum particles measuring one thing like the colour can change something you already measured previously like the side. Or, to go back to the electron, measuring the spin in one direction makes the spin in the other direction maximally uncertain.

Therefore, if you use quantum mechanics to calculate the result of the fourth measurement, you get the correct answer that agrees with observations. Loosely speaking this is because if you multiply these measurement results and want to remove the ones that square to one, you need to change their order. And if you do that, in quantum mechanics, this can bring in a minus.

But the most interesting thing about this paradox is that the final result, the -1, is not probabilistic. It’s deterministic, it follows from the previous ones. Once you have the results of the first three measurements, you know the result of the fourth.

And this has practical use. It means that if you can generate three electrons and make those measurements, that will always and reliably produce this weird quantum state. And it’s this quantum weirdness that gives quantum computers their extra edge. This is why physicists are so keen on finding reliable ways to generate it, because it means additional computational power.

And this finally brings me to the paper with the 37 dimensions. The authors first provide mathematical proof that one doesn’t need 4 measurements to find a contradiction, as we did. They say one can do it with merely 3. And then they experimentally construct a way to do that.

This is where the 37 dimensions come in. Those are not dimensions of space like the space around us. The 37 dimensions are the dimensions of the Hilbert space for six photons. This means basically that taken together the photons have 37 different measurable properties, polarization, phases, intensity, and so on.

So it’s not as mysterious as it sounds. Sorry. But isn’t this always the case with quantum physics? That if you look at the maths, it’s not as mysterious as it sounds?

Why 37? That’s just the lowest number of dimensions that they found to work. Maybe tomorrow someone will do it in 19. And, no I am not going to draw you a table in 37 dimensions, I have my limits.

Let me instead add some comments on what this is all good for. The advantage of quantum computers comes from them having properties that a non-quantum system cannot have. And these paradoxical GHZ states are a great example for this because if you know how to generate them, they are so reliable. And the fewer measurements you need to make the more efficient your calculation becomes, and 3 is less than 4. On the flipside you now have to deal with these 37 dimensions, so don’t expect this to go into mass production tomorrow.

On a more practical note. If you are working on your own theory of quantum physics, and I know there are quite a few among you who dream of that, you need to come up with a way to obtain the correct results for the GHZ experiment.

And then let me finally say some words about how superdeterminism explains the quantum mechanical result. Superdeterminism is an unfortunate term that John Bell used to describe what he thought was an implausible explanation. What it really means is just that the probability of a measurement outcome depends on what you measure. In the GHZ table this means that for example the result for the side of the second coin in the third measurement can differ from the one in the first measurement, because the measurements on the other coins are different. The result depends on the context.

The benefit of superdeterminism, and the reason why I am convinced it’s the correct explanation, is that it is local and therefore compatible with Einstein’s theory. Superdeterminism has no “spooky action at a distance.” Indeed, we know from Bell’s theorem that it’s the \*only way to make the results of quantum mechanics compatible with Einstein’s locality.

People don’t like this explanation because they think it’s constraining their free will or something. But the way that I think about it is just that it’s a consistency requirement. And yes, I am working on a few more papers about this, it’s just that these videos keep getting in the way. But hey, at least we learned something: how to give Sabine an existential crisis.

![Photo showing a close up of the Majorana 1 quantum chip with brass equipment in the background.](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c34d4de-628c-4101-b8d3-e9eb018206d5_2000x732.jpeg)

*Image: Microsoft*

Last week, Microsoft announced they’d made a major breakthrough in quantum computing. The said they’d achieved the first step for a scalable platform that could bring us to a million qubits quickly, and unleash the power of quantum computing onto the world. I’ve had a look.

[On February 19th, Microsoft introduced Majorana 1](https://news.microsoft.com/source/features/ai/microsofts-majorana-1-chip-carves-new-path-for-quantum-computing/), which, according to their press release is “the world’s first quantum chip powered by a new Topological Core architecture”. With this, they say, they expect to “realize quantum computers capable of solving meaningful, industrial-scale problems in years, not decades.”

The CEO of Microsoft, Satya Nadella, further [wrote on X-Twitter](https://x.com/satyanadella/status/1892242895094313420) that they could make this work because “after a nearly 20 year pursuit, \[they’ve\] created an entirely new state of matter” and that they “now have a clear path to a million-qubit processor.” At the same time, the Microsoft team also published [a paper in Nature](https://www.nature.com/articles/s41586-024-08445-2) and a [roadmap on the arXiv](https://arxiv.org/abs/2502.12252). The news made [international](https://www.nytimes.com/2025/02/19/technology/microsoft-quantum-computing-topological-qubit.html) headlines quickly.

Okay, let’s unravel this. Quantum computers can solve certain mathematical problems much faster than a conventional computer, at least in theory. In practice, the already existing quantum computers are so small they can’t do anything of practical use. Quantum computers operate on quantum bits, qubits, and estimates say that for practical uses we’d need to reach about a million. In reality, we are currently at 150 or so useful qubits. I say this so carefully because there are chips with more qubits than that, but I have seen no evidence that they can actually calculate with all of them.

There are many different ways to create qubits and at the moment there is a lot of competition between them. Microsoft, in particular, has worked on “topological quantum computing,” which is the path least taken and also what the new announcement is about. Google and Nokia are also working on topological quantum computing, but progress has generally been slow.

For topological quantum computing, you need quantum states whose properties which are protected by conserved geometrical features. Topological quantum states are emergent. They don’t exist as properties of particles, like, say, spin, but they are properties of many interacting particles.

To create a topological qubit, you need to find a suitable material and a suitable configuration in which you get these conserved states. This has proved to be extremely difficult. But the nice thing about topological qubits is that, because their features are conserved, they are very robust to noise. Microsoft uses a particular type of topological state, called a “Majorana zero mode,” which their new computing platform is named after.

To create these states, Microsoft uses topological superconductors, which are not a new discovery. They’re also not a “state of matter,” but rather a quantum phase of the solid state. Though to be honest, this is picking on words. Personally I don’t mind if they call it a state of matter, but I would object to calling it a new state.

New is that they claim that they now have something to actually calculate with. In their Nature paper, they report that they managed to create such a topological qubit with two different states, that are parity states, and that they could measure and distinguish these two different states with 99% reliability. This is pretty good. They did this with tiny aluminum wires cooled to about 50 milli-Kelvin.

However, to demonstrate that you have a topological qubit, you don’t just need to show that it has two states. Other qubits also have that. You need to show that you can perform operations on it that are protected by the topological properties. That is, they have shown that they have a qubit. They have not shown that it’s topological.

[A lot of physicists are skeptical about this after a Microsoft-led team published a paper in 2018](https://www.nature.com/articles/d41586-021-00612-z) that was retracted in 2021. The authors admitted to having made mistakes in their data analysis and presentation.

[According to a Nature News piece](https://www.nature.com/articles/d41586-025-00527-z) that accompanied the new Microsoft paper, Steven Simon, from the University of Oxford, said “Would I bet my life that they’re seeing what they think they’re seeing? No, but it looks pretty good.” And I think that’s a fair summary. Yes, it is really good news, and it looks very promising, but the news isn’t remotely as big as the headlines made it appear.

[As to the Microsoft roadmap](https://arxiv.org/abs/2502.12252). It basically says that once they have one qubit, they will put a lot of them together. The problem with doing this is that you need to still cool them all to a few milli-Kelvin and the larger the qubits are, the more difficult this becomes.

But based on the Microsoft roadmap to quantum computing, I can now give you a scientifically accurate roadmap to becoming a millionaire. Step 1: Make 100 dollars. Step 2: Repeat Step one a 10,000 times.