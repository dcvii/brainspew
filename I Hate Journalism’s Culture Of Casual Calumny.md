---
title: "I Hate Journalism’s Culture Of Casual Calumny"
source: "https://jessesingal.substack.com/p/i-hate-journalisms-culture-of-casual?publication_id=4833&post_id=179367755&isFreemail=false&r=7br8e&triedRedirect=true"
author:
  - "[[Jesse Singal]]"
published: 2025-11-19
created: 2025-11-19
description: "This isn’t what we’re supposed to be doing"
tags:
  - "clippings"
---
### This isn’t what we’re supposed to be doing

![](https://substackcdn.com/image/fetch/$s_!-SS7!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb57d535b-2754-4412-aa8d-fb628f2fbdad_2121x1414.jpeg)

Earlier this week, Taylor Lorenz [sent out a newsletter](https://www.usermag.co/p/anti-ai-grifters-college-football) accusing Eliezer Yudkowsky and Nate Soares of being “anti-AI grifters.” It neatly captures a style of journalism that drives me crazy.

Lorenz’s newsletter begins:

> On Friday, there was a big event in L.A. for [a widely-panned](https://substack.com/redirect/341bda1d-81b0-4803-9a64-192feebf0f68?j=eyJ1IjoiZGZuayJ9.IP66IkeSFyWKVcBLzkwXtiictTIgyqQqWwmv-Fbyfb0) book supposedly critiquing the AI industry called *If Anyone Builds It, Everyone Dies* by Eliezer Yudkowsky and Nate Soare *s*. I don’t cover the nuances of the AI industry much, but the PR person for the book invited me and I said ‘sure.’ I and a few other tech reporters RSVPd and received confirmation that we’d be attending. A few days later, however, the PR person called and said, actually, I was DISINVITED for being too critical of Silicon Valley. Apparently my reporting on tech billionaires had angered the organizers.
> 
> I thought this was pretty wild. I’m broadly critical of Silicon Valley, but I’m also very pro-technology, especially compared to others on the left. I believe in a better world through technology, just not technology controlled by the current class of Silicon Valley billionaires. I’ve never been disinvited from a book event for my tech criticism, and myself and a bunch of tech journalists laughed about it.

It was very silly for the organizers of this party to invite and then disinvite Lorenz; they’re basically asking for negative coverage, and Lorenz has every right to go public about this strange turn of events. What bothers me is the way Lorenz can’t just tell the story, but seems intent on inflicting maximum reputational damage on Yudkowsky, Soares, and other people in their orbit — she just takes every last possible swipe at this crew, many of them unfair. This is part of what I call the culture of casual calumny in journalism, and it really sucks.

The CCC is a style of journalism and punditry (these days, the two bleed into one another more often than not, including in this newsletter) that is very attack-dog, very *fuck these guys*, very quick to render judgment and to privilege accusation over levelheaded attempts at understanding and explanation. It could be seen as a natural outgrowth of what [I call rightside journalism](https://jessesingal.substack.com/p/rightside-norms-accuracy-norms-and), or journalism geared more at demonstrating one’s ideological bona fides than getting at the truth or explaining the world. It’s natural, if you write from the stance of needing to demonstrate that you are on the “right” side of various issues, that along the way you’re going to get in the habit of broadcasting your own righteousness, which inevitably entails accusing others of lacking in this department.

The example provided by Lorenz is far from unique, but it does stand out because of how uncharitable she is and how many accusations she packs into one short section of one newsletter.

To be clear, even calling Yudkowsky and Soares “anti-AI grifters” who are in bed with big tech makes very little sense. Their book is, in part, a call for some of the most powerful companies and governments in the world to stop a line of research that could generate many billions of dollars (he said, [nervously checking his Robinhood account every few minutes](https://www.theguardian.com/business/live/2025/nov/18/stock-market-sell-off-ai-bubble-google-nvidia-ftse-100-bitcoin-business-live-news)).

The book’s argument is *extremely* inconvenient for the tech industry, especially given that it hit the *New York Times* bestseller list. If you wanted to maximize your own money via demagoguery — if you wanted to *grift* — you certainly wouldn’t do what Yudkowsky and Soares are doing.[^1] Plus, if you’re even remotely familiar with Yudkowsky’s biography, it’s quite clear he’s a true believer, whatever you think of those beliefs. A true believer is the opposite of a grifter.

Here’s the next graf from Lorenz:

> Despite the best efforts of their PR person, one tech reporter did manage to sneak into the event without getting banned. At the party, she attempted to chat with Nate Soares, one of the authors of the book about his theories. He snapped at her, turned his back, walked away and refused to speak to her. Apparently, he only converses with people who have [read and abide by a 28 page Google doc he made outlining the rules of speaking with him](https://docs.google.com/document/d/1o1JaH2nxbpfHgpx25dwJnQ3dQAPikmlqrn6c9gNP9gE/edit?tab=t.0#heading=h.6o6m4x2z3grc).

When I read that last sentence, I thought to myself *That’s not true*. It was, I admit, a knee-jerk response. I don’t know much about Soares and I hadn’t heard of this document. But Lorenz’s claim that Soares speaks only to people who first read a 28-page Google Doc made him sound like a cartoon villain of an aloof tech bro, not a flesh-and-blood human, and generally (not always) when someone is described in such terms, the description is inaccurate.

I clicked on the document. It isn’t a set of rules you *need to* abide by to be granted the privilege of talking to Nate Soares. It’s an *extremely* [rationalist](https://en.wikipedia.org/wiki/Rationalist_community) \-coded “handbook.” “There’s a pattern to the ways that conversations with me go poorly (when they go poorly at all),” the “Purpose” section explains. “If my conversation partners know what those patterns are, they’re easier to avoid.” The handbook comes off as a bit kooky, sure, but less so if you’re familiar with the rationalists. They’re obsessed with clear thinking and communication, and they’ve developed a related obsession with “failure modes,” or with naming the specific ways thinking and communicating can go wrong. They basically (to oversimplify) take an engineering approach to many features of human life and society that people tend to take for granted, or to not think all that deeply about. To those unfamiliar with this style of thinking, it can come across as weird or stilted. (I say this as a general fan and sympathizer of a lot of what rationalists do, albeit with some natural caveats.)

Coincidentally, on Sunday night I was at an event on AI with a group that included my friend Andy Mills. Mills and some colleagues have produced an [excellent ongoing podcast about AI called](https://podcasts.apple.com/us/podcast/the-last-invention/id1839942885) *[The Last Invention](https://podcasts.apple.com/us/podcast/the-last-invention/id1839942885)* that I highly recommend. He interviewed Soares on it for [the episode they did](https://open.spotify.com/episode/2usKKxXM8gkw2mlTNxLoJQ) on the “doomers.” And, as I found out when I mentioned to him that I might write this post, he was actually at the party Lorenz was uninvited from.

Mills responded to Lorenz’s account with incredulity. He noted that when he interviewed Soares for his podcast, “there were no attempts to get me to sign anything \[and\] no promises were made beforehand.” “In fact, one of the questions I asked Nate was if he’d be willing to do a public debate with people who disagree with him and he was open to it.”

For what it’s worth, I emailed Soares and he said he didn’t remember anything occurring like what Lorenz alleged:

> Huh! I don’t recall anyone at the event introducing themselves as a tech reporter. I don’t recall snapping at anyone or refusing to speak with anyone. I expect I would remember that if it had happened, and so I suspect it just didn’t happen. In general, I am happy to speak with tech reporters, and I’m friendly with many...
> 
> (And as you allude to, the google doc doesn’t lay out any “rules” to abide by, but rather lays out suggestions and notes and experiences that people might find helpful.)

Soares quickly sent me a follow-up email, seemingly worried he was being unfair, suggesting I might want to cut the line “I suspect it didn’t happen”:

> On reflection I’m uncertain, there were a lot of convos. I’m confident that I didn’t do any “snapping” and that I didn’t have any ill will towards any attendee, but someone might’ve intro’d themself as a tech reporter and decided to interpret a polite exit (to go greet other guests) as a spurn. I don’t remember it, but I was greeting a lot of people and it could’ve happened.

In any case, Lorenz continues in her newsletter:

> The whole situation is such a perfect encapsulation of how full of nonsense many of these AI “critics” are. While they posture as being against AI, they’re deeply in bed with Silicon Valley billionaires. The *If Anyone Builds It, Everyone Dies* book event was co-hosted by Grimes and Aella, a Silicon Valley OnlyFans girl and aspirational blogger who is most well known for posting thirst traps on Twitter and being praised by Marc Andreessen. None of these people can handle even the slightest criticism of capitalism or the tech industry, because they are deeply embedded in it.

The level of unwarranted shit-flinging here is off the charts. Saying that nobody involved in the writing or promotion of *If Anyone Builds It, Everyone Dies* “can handle even the slightest criticism of capitalism or the tech industry” is roughly like saying no one involved in an NBA podcast ever criticizes NBA players. It’s very, very close to the opposite of the truth.

Without getting too deep into the weeds of *If Anyone Builds It*, a quick example. One of Yudkowsky and Soares’ key arguments is that already, we humans don’t really understand how our AI systems work. They are too complex and are constantly surprising us with unexpected [emergent](https://en.wikipedia.org/wiki/Emergence) behaviors. This problem is going to get worse, not better, as the systems get much bigger and more complex. They believe that many of the leading lights in tech and AI are deeply Pollyannaish about our ability to keep tomorrow’s AI systems on a human-friendly leash. To support this fact, they include a pair of quotes by Elon Musk and leading AI researcher Yann LeCun (who happened to be one of the speakers onstage at the event we were at the other night). I found these quotes quite disturbing — they jumped out at me because they exhibited a level of confidence that seems quite unwarranted.[^2]

Yudkowsky and Soares then write:

> You’re living in a world where Musk’s idealistic plans and LeCun’s vague assurances were *not* met by an outrush of horror from the rest of academic science and industry’s engineers.
> 
> Imagine if somebody like that, with enough money and power to make their wishes real, announced they were building a nuclear power plant based on that level of theory! Imagine the reactions of the competent veterans who knew it was hard, who could analyze the resulting disaster using mature engineering techniques!
> 
> If there aren’t thousands of horrified scientists and engineers leaping up to beg governments to shut down those particular AI labs, it tells you that it’s not just a problem of individuals. It means that whole field of science is in the stage of folk theory and blind optimism.

I’m not even commenting on the merits of these very complicated debates; I’m pointing out how ridiculous — how calumnious — it is to level the charge against this crew that they refuse to criticize the tech industry. Yudkowsky and Soares are calling for some of the tech industry’s labs to be shut down and are calling out, by name, some of its most important figures!

Lorenz describes [Aella](https://aella.substack.com/) (a [onetime](https://www.blockedandreported.org/p/episode-44-lets-talk-about-sex-and-bd6) *[BARPod](https://www.blockedandreported.org/p/episode-44-lets-talk-about-sex-and-bd6)* [guest](https://www.blockedandreported.org/p/episode-44-lets-talk-about-sex-and-bd6)) as “a Silicon Valley OnlyFans girl and aspirational blogger who is most well known for posting thirst traps on Twitter and being praised by Marc Andreessen.” *Marc Andreessen praised her!* This is a ridiculously unfair gloss on Aella’s online presence, whatever you think about Aella! If you’re unfamiliar with Aella and want to know *why* it’s ridiculous, [listen to PJ Vogt’s interview with her on his podcast](https://www.searchengine.show/how-does-a-rationalist-make-a-baby) *[Search Engine](https://www.searchengine.show/how-does-a-rationalist-make-a-baby)*. Calling her an “aspirational blogger” is rich given that Aella’s Substack is, for what it’s worth, significantly larger than Lorenz’s (and *much* larger than mine, not that anyone’s counting). (Maybe a side note, but if we’re talking about these folks’ relationships to the tech industry, it feels weird that Lorenz didn’t even mention that Grimes is the mother of three of Musk’s children and that [there has been a lot of drama between the two parents.](https://www.glamour.com/story/grimes-elon-musk-their-upsetting-online-conflict-explained))

As a final critique of those evil grifting bastards who uninvited her from a party, Lorenz mentions that Émile P. Torres “ [writes](https://www.truthdig.com/articles/under-a-mask-of-ai-doomerism-the-familiar-face-of-eugenics/?utm_source=substack&utm_medium=email) about how the authors are deeply intertwined with the eugenics movement,” and then excerpts some of an article he published in *TruthDig*.

That’s a very specific, very serious charge. Neither the excerpt Lorenz publishes nor the article itself supports it. Rather, Torres argues something a bit more technical: He thinks that Yudkowsky’s transhumanist beliefs, which include the desire to see a version of human consciousness transcend our human bodies and colonize the stars (this is also pretty common in rationalist circles), is a version of eugenics because it entails leaving “unaugmented” humans behind and acknowledging that they will be born disadvantaged relative to their augmented peers (at one point Yudkowsky mentioned that these babies will be seen as “crippled,” relatively speaking). This stuff all gets pretty weird, pretty fast, and is fodder for many interesting debates. But it’s also pretty damn different from “intertwined with the eugenics movement” in the sense that Lorenz’s readers are going to understand that phrase — as something like “Yudkowsky and Soares are involved with people and organizations that seek to breed out ‘undesirable’ traits from humans.”

Details aren’t the point here, though: The point is clearly to inflict maximum reputational damage on Yudkowsky and Soares and anyone in their orbit.

### Sucky Incentives

Taylor Lorenz is, of course, far from the only pundit to write in this manner. Anyone who reads this newsletter or who listens to *BARPod* will be able to think of countless other examples of the culture of casual calumny in journalism — of journalists recklessly attacking the reputations of both other journalists and random people who were caught in the wrong place at the wrong time.

Over time, more and more journalism can be considered to suffer from the CCC. There are material advantages to engaging in this sort of writing versus its slower, more thoughtful alternatives. Plus, while the line between journalists and pundits has been blurry for a while, increasingly the line between journalists, pundits, and “influencers” is blurry as well. The vast majority of influencers cannot gain and retain sufficient attention unless they are sparking, stoking, and responding to beefs — both theirs and others’. This particular beef will likely die down, but if, say, some group from the doomer camp subsequently criticized Lorenz on social media, that would be a feature, not a bug. She could then write a post or make a video about the response to the response to her article, which would potentially stoke more conflict and drama, and so on.

I definitely don’t have clean hands here. I’m positive that over the years I’ve engaged in the culture of casual calumny. My 2014–2017 years at *New York* magazinewere truly formative for me, and taught me that I love *actual* journalism and *actual* reporting. But whatever good habits I picked up during that period, it was also when I first became a daily Twitter user. It’s impossible to overstate the impact Twitter had on journalists back then, and it was a prime vector for the CCC. I’m 100% positive that I was overly snarky, quick to judge, and that in some cases I joined the shovel brigades in heaping mud on the names of people I knew almost nothing about. (Consider this a blanket apology, and an open invitation to criticize me if I do it again.)

Things are much worse now for journalism, but *influencing* might still be something of a growth industry (albeit a very top-heavy, winner-take-all one — 99% of people who try to become influencers as a career will fail). There’s not much I can do about the decision influencers make about how fair to be to their perceived adversaries and, again, all the incentives point in an ugly direction.

But journalism still exists, even if more and more of us are both journalists and something else (pundits, influencers, whatever). So we have agency with regard to the question of how we’re going to relate to our sources and subjects and others. The culture of casual calumny is a noxious style that makes *actual* journalism harder to do, because if you see it as your job to constantly render public judgments on the various people you are writing about, that’s going to have the natural effect of making you less inquisitive and less capable of traversing the complexity and nuance that any decent journalist is going to come across in their work, sooner or later.

I mentioned Andy Mills and PJ Vogt already. Listen to their approach to [covering AI (Mills)](https://podcasts.apple.com/us/podcast/the-last-invention/id1839942885) and [Aella (Vogt)](https://www.searchengine.show/how-does-a-rationalist-make-a-baby) and note the complete lack of casual calumny. Part of the reason I appreciate Mills and Vogt is that they have opted out of this culture entirely. Or maybe “opted out” is the wrong phrase given that they never really participated in it, but my point is that even though everyone I’m talking about here goes by the title “journalist,” there are wildly different approaches to the same trade.

You don’t *have* to be a jerk. You *can* take seriously what it means to criticize someone else.

---

*Questions? Comments? Wildly unwarranted criticisms? I’m at [singalminded@gmail.com](https://jessesingal.substack.com/p/) or on X at [@jessesingal](https://twitter.com/jessesingal). Image: The shadow of two young boys fighting projected against a red brick wall via Getty.*

[^1]: I actually want to sidestep the question of the merits of *If Anyone Builds It* in this particular newsletter, other than to say I read it, found it broadly convincing, but also know that there are [people whose views I trust who were less convinced](https://www.understandingai.org/p/the-case-for-ai-doom-isnt-very-convincing). I felt like I needed to read it to better understand my own fears about our AI future, but that I wasn’t quite informed enough to offer a substantive public “review” or anything like that. Maybe in the future, if there’s any demand from it among my readers, I’ll at least peel off a couple of the specific critiques of the book published by others and address them.

[^2]: Here’s how the two men are quoted by Yudkowsky and Soares:

**Musk**:

I’m going to start something called TruthGPT. Or a maximum truth-seeking AI that tries to understand the nature of the universe.

I think this might be the best path to safety, in the sense that an AI that cares about understanding the universe is unlikely to annihilate humans, because we are an interesting part of the universe.

**LeCun**, in a series of quotes apparently pulled from different places:

Calm down. Human-level AI isn’t here yet. And when it comes, it will not want to dominate humanity. Even among humans, it is not the smartest who want to dominate others and be the chief.

———

Because they would have no desire to do anything else. Why? Because we will engineer their desires.

———

My benevolent defensive AI will be better at destroying your evil AI than your evil AI will be at hurting humans.

———

We can design AI systems to be both superintelligent *and* submissive to humans.