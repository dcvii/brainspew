---
title: "Thinking, Simulated"
source: "https://onepercentrule.substack.com/p/thinking-simulated?publication_id=3028809&post_id=160876371&isFreemail=true&r=7br8e&triedRedirect=true"
author:
  - "[[The One Percent Rule]]"
published: 2024-10-26
created: 2025-04-08
description: "The Chinese Room"
tags:
  - "clippings"
---
### The Chinese Room

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0f30bd09-e3b0-4c44-8512-5dd2f8e8140c_720x590.png)

Many of my students refer to AI as “he” or “she”. Some of them clearly get ‘emotionally’ attached. I remind them that the belief that computers think is a category mistake, not a breakthrough. It confuses the appearance of thought with thought itself. A machine mimicking the form of human responses does not thereby acquire the content of human understanding.

Artificial intelligence, despite its statistical agility, does not engage with meaning. It shuffles symbols without knowing they are symbols. [John Searle](https://en.wikipedia.org/wiki/John_Searle), who is now 92 years old, pointed this out with a clarity that still unsettles mainstream confidence in the computational theory of mind.

## Syntax and Semantics

In a culture eager to believe that minds are software running on biological hardware, Searle's intervention is jarring: minds have semantics; computers do not.

No amount of syntactic precision will bridge that gap. His [Chinese Room](https://plato.stanford.edu/entries/chinese-room/) thought experiment doesn’t seek to reduce computation; it reveals a basic asymmetry between simulation and understanding.

Imagine a man locked in a room, receiving Chinese symbols and using a rulebook in English to produce replies. To outsiders, his answers look fluent. But he doesn't understand a word, he’s rearranging tokens. This is not an analogy; it is an argument about necessary conditions. **Computation is not sufficient for understanding. Syntax is not semantics.**

## Counterarguments and Misreadings

Critics, of course, have pressed back. The most famous counter, “ [the systems reply](https://philosophy.tamucc.edu/notes/chinese-room-criticisms-and-replies) ”, claims that while the man doesn’t understand Chinese, the system as a whole might. Perhaps, they suggest, understanding is emergent at the system level. **But this simply relocates the ignorance**. The system is composed entirely of meaningless symbol manipulations; **where, in that inert machinery, is the leap to meaning?** No component understands, and no architecture, however intricate, conjures understanding from zero. Emergence isn’t magic.

There’s another worry: that insisting on biology implies chauvinism, as if only carbon-based matter can think. But this is a misreading. The point is not that biology matters, but that the causal powers of biological systems matter. If a machine someday acquires those powers, if it causes understanding in the way brains do, then it too might have a mind. But current systems, for all their performance, lack that causal depth. Their failure isn’t rooted in their being machines; it lies in their inability to produce the kinds of causal processes that generate understanding.

## Mistaken Comforts

**The mind is not fast computation. It is structured experience.** Human neurons are magical; they are the substrate that, thus far, happens to work. If another structure emerges that carries the same semantic weight, then we have a new kind of mind. But we should not pretend that generating output approximating human language implies any such thing has arrived.

There is aesthetic comfort in imagining minds as code, neat, legible, transferable. It enables the fantasy that we can upload ourselves, cheat death, or reproduce consciousness by sufficient layering of transformer models. **But code is not soul. It is not even self.**

## Mistaking Simulation for Thought

None of this is to deny that machines can be intelligent in a limited, instrumental sense. They can strategize, calculate, adapt. But they do not know that they are doing so. They lack what philosophers call [intentionality](https://plato.stanford.edu/entries/intentionality/), the aboutness of thought, the directedness of experience.

**They lack the inner horizon in which thoughts refer, feelings register, and pain hurts.**

This is the terrain we abandon if we declare minds reducible to programs. We lose not just explanatory depth but the very thing being explained. We confuse the shadows on the cave wall for the fire that casts them.

## What Searle Reminds Us

Searle's provocation, then, is not a Luddite lament. It is a reminder: the question is not whether we can build machines that simulate intelligence. We already have. **The question is whether we understand what it is they are simulating, and whether in confusing the simulation for the thing, we risk forgetting what it means to think at all.**

If we forget, it will not be because machines fooled us. It will be because we preferred the comfort of mimicry to the burden of thinking and understanding.

Stay curious

Colin