---
title: "Four ways learning Econ makes people dumber re: future AI â€” LessWrong"
source: "https://www.lesswrong.com/posts/xJWBofhLQjf3KmRgg/four-ways-learning-econ-makes-people-dumber-re-future-ai"
author:
  - "[[Steven Byrnes]]"
published: 2025-08-21
created: 2025-09-30
description: "Thereâ€™s a funny thing where economics education paradoxically makes people DUMBER at thinking about future AI. Econ textbooks teach concepts & frames that are great for most things, but counterproductive for thinking about AGI. Here are 4 examplesâ€¦"
tags:
  - "clippings"
---
([*Cross-posted from X*](https://x.com/steve47285/status/1958527894965108829)*, intended for a general audience.*)

Thereâ€™s a funny thing where economics education paradoxically makes people DUMBER at thinking about future AI. Econ textbooks teach concepts & frames that are great for most things, but counterproductive for thinking about AGI. Here are 4 examples. Longpost:

**THE FIRST PIECE** of Econ anti-pedagogy is hiding in the words â€œlaborâ€ & â€œcapitalâ€. These words conflate a superficial difference (flesh-and-blood human vs not) with a bundle of unspoken assumptions and intuitions, which will all get broken by Artificial General Intelligence (AGI).

By â€œAGIâ€ I mean here â€œa bundle of chips, algorithms, electricity, and/or teleoperated robots that can autonomously do the kinds of stuff that ambitious human adults can doâ€”founding and running new companies, R&D, learning new skills, using arbitrary teleoperated robots after very little practice, etc.â€

Yes I know, this does not exist yet! (Despite hype to the contrary.) Try asking an LLM to autonomously write a business plan, found a company, then run and grow it for years as CEO. Lol! It will crash and burn! But thatâ€™s a limitation of todayâ€™s LLMs, not of â€œall AI foreverâ€. AI that could nail that task, and much more beyond, is obviously possibleâ€”human brains and bodies and societies are not powered by some magical sorcery forever beyond the reach of science. I for one expect such AI in my lifetime, for better or worse. (Probably â€œworseâ€, see below.)

Now, is this kind of AGI â€œlaborâ€ or â€œcapitalâ€? Well itâ€™s not a flesh-and-blood human. But itâ€™s more like â€œlaborâ€ than â€œcapitalâ€ in many other respects:

- Capital canâ€™t just up and do things by itself? AGI can.
- New technologies take a long time to integrate into the economy? Well ask yourself: how do highly-skilled, experienced, and entrepreneurial immigrant humans manage to integrate into the economy immediately? Once youâ€™ve answered that question, note that AGI will be able to do those things too.
- Capital sits around idle if there are no humans willing and able to use it? Well those immigrant humans donâ€™t sit around idle. And neither will AGI.
- Capital canâ€™t advocate for political rights, or launch coups? Wellâ€¦

Anyway, people see sci-fi robot movies, and they get this! Then they take economics courses, and it makes them dumber.

(Yes I know, #NotAllEconomists etc.)

**THE SECOND PIECE** of Econ anti-pedagogy is instilling a default assumption that itâ€™s possible for a market to equilibrate. But the market for AGI cannot: AGI combines a property of labor markets with a property of product markets, where those properties are mutually exclusive. Those properties are:[^1]

- (A) â€œNO LUMP OF LABORâ€: If human population goes up, wages drop in the very short term, because the demand curve for labor slopes down. But in the longer term, people find new productive things to doâ€”the demand curve moves right. If anything, the value of labor goes UP, not down, with population! E.g. dense cities are engines of growth!
- (B) â€œEXPERIENCE CURVESâ€: If the demand for a product rises, thereâ€™s price increase in the very short term, because the supply curve slopes up. But in the longer term, people ramp up manufacturingâ€”the supply curve moves right. If anything, the price goes DOWN, not up, with demand, thanks to economies of scale and R&D.

QUIZ: Considering (A) & (B), whatâ€™s the equilibrium price of this AGI bundle (chips, algorithms, electricity, teleoperated robots, etc.)?

â€¦Trick question! There is no equilibrium. Our two principles, (A) â€œno lump of laborâ€ and (B) â€œexperience curvesâ€, make equilibrium impossible:

- If price is low, (A) says the demand curve races rightwardsâ€”thereâ€™s no lump of labor, therefore thereâ€™s massive profit to be made by skilled entrepreneurial AGIs finding new productive things to do.
- If price is high, (B) says the supply curve races rightwardsâ€”thereâ€™s massive profit to be made by ramping up manufacturing of AGI.
- If the price is in between, then the demand curve and supply curve are BOTH racing rightwards!

This is neither capital nor labor as we know it. Instead of the market for AGI equilibrating, it forms a positive feedback loop / perpetual motion machine that blows up exponentially.

Does that sound absurd? Thereâ€™s a precedent: humans! The human world, as a whole, is already a positive feedback loop / perpetual motion machine of this type! Humans bootstrapped themselves up from a few thousand hominins to 8 billion people running a $80T economy.

How? Itâ€™s not literally a perpetual motion machine. Rather, itâ€™s an engine that draws from the well of â€œnot-yet-exploited economic opportunitiesâ€. But remember â€œNo Lump of Laborâ€: the well of not-yet-exploited economic opportunities is ~infinitely deep. We havenâ€™t run out of possible companies to found. Nobody has made a Dyson swarm yet.

Thereâ€™s only so many humans to found companies and exploit new opportunities. But the positive feedback loop of AGI has no such limit. The doubling time can be short indeed:

Imagine an autonomous factory that can build an identical autonomous factory, which then build two more, etc., using just widely-available input materials and sunlight. Economics textbooks donâ€™t talk about that. But biology textbooks do! A cyanobacterium is such a factory, and can double itself in a day (â‰ˆ googol percent annualized growth rate ðŸ˜›).

Anyway, we donâ€™t know how explosive will be the positive feedback loop of AGI building AGI, but I expect it to be light-years beyond anything in economic history.

**THE THIRD PIECE** of Econ anti-pedagogy is its promotion of GDP growth as a proxy for progress and change. On the contrary, itâ€™s possible for the world to transform into a wild sci-fi land beyond all recognition or comprehension each month, month after month, without â€œGDP growthâ€ actually being all that high. GDP is a funny metric, and especially poor at describing the impact of transformative technological revolutions. (For example, if some new tech is inexpensive, and meanwhile other sectors of the economy remain expensive due to regulatory restrictions, then the new tech might not impact GDP much, no matter how much it upends the world.) I mean, sure we can argue about GDP, but we shouldnâ€™t treat it as a proxy battle over whether AGI will or wonâ€™t be a big deal.

Last and most importantly, **THE FOURTH PIECE** of Econ anti-pedagogy is the focus on â€œmutually-beneficial tradesâ€ over â€œkilling people and taking their stuffâ€. Econ 101 proves that trading is selfishly better than isolation. But sometimes â€œkilling people and taking their stuffâ€ is selfishly best of all.

When weâ€™re talking about AGI, weâ€™re talking about creating a new intelligent species on Earth, one which will eventually be faster, smarter, better-coordinated, and more numerous than humans.

Normal people, people who have seen sci-fi movies about robots and aliens, people who have learned the history of colonialism and slavery, will immediately ask lots of reasonable questions here. â€œWhat will their motives be?â€ â€œWho will have the hard power?â€ â€œIf theyâ€™re seeming friendly and cooperative early on, might they stab us in the back when they get more powerful?â€

These are excellent questions! We should definitely be asking these questions! (FWIW, this is my area of expertise, and Iâ€™m very pessimistic.)

â€¦And then those normal people take economics classes, and wind up stupider. They stop asking those questions. Instead, they â€œlearnâ€ that AGI is â€œcapitalâ€, kinda like an injection-molding machine. Injection-molding machines wouldnâ€™t wipe out humans and run the world by themselves. So weâ€™re fine. Lol.

## Tweet 2

â€¦Since actual AGI is so foreign to economistsâ€™ worldviews, they often deny the premise. E.g. hereâ€™s Tyler Cowen demonstrating a complete lack of understanding of what we doomers are talking about, when we talk about future powerful AI.

![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/xJWBofhLQjf3KmRgg/haflfoxcruloiklj0mdo)

[Source](https://marginalrevolution.com/marginalrevolution/2025/02/why-i-think-ai-take-off-is-relatively-slow.html)

## Tweet 3

And hereâ€™s Daron Acemoglu assuming without any discussion that in the next 10 yrs, â€œAIâ€ will not include any new yet-to-be-developed techniques that go way beyond todayâ€™s LLMs. Funny omission, when the whole LLM paradigm didnâ€™t exist 10 yrs ago!

(Tbc, itâ€™s fine to make that assumption! Maybe it will be valid, or maybe not, who knows, technological forecasting is hard. But when your paper depends on a giant load-bearing assumption about future AI tech progress, an assumption which many AI domain experts dispute, then that assumption should at least be clearly stated! Probably in the very first sentence of the paper, if not the title!)

![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/xJWBofhLQjf3KmRgg/oewufhnczsyh1enzdnkc)

Source: â€œThe Simple Macroeconomics of AIâ€ (Acemoglu, 2024)

## Tweet 4

And hereâ€™s another example of economists â€œarguingâ€ against AGI scenarios by simply rejecting out of hand any scenario in which actual AGI exists. Many such examplesâ€¦

![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/xJWBofhLQjf3KmRgg/lhcf9yg0otltn0yaojy6)

[Source](https://x.com/steve47285/status/1771187467229511718)

## Tweet 5

I think part of the problem is people taking human brains for granted instead of treating them as an existence proof that todayâ€™s LLMs are nowhere near the ceiling of whatâ€™s possible with AI â†“ *(*[*source*](https://www.lesswrong.com/posts/yew6zFWAKG4AGs3Wk/foom-and-doom-1-brain-in-a-box-in-a-basement)*)*

> ## 1.3.2 Three increasingly-radical perspectives on what AI capability acquisition will look like
> 
> Here are three perspectives:
> 
> 1. *Economists and other people who see AI as a normal technology:* â€œIf we want AI to work in some new application area, like some particular industrial design workflow, then humans need to do a lot of R&D work to develop and integrate the AI into this task.â€
> 2. *LLM-focused AGI person:* â€œAh, thatâ€™s true today, but eventually other AIs can do this â€˜development and integrationâ€™ R&D work for us! No human labor need be involved!â€
> 3. *Me:* â€œNo! Thatâ€™s still not radical enough! In the future, that kind of â€˜development and integrationâ€™ R&D work just wonâ€™t need to be done at allâ€”not by humans, not by AIs, not by anyone! Consider that there are 8 billion copies of basically *one* Â human brain design, and if a copy wants to do industrial design, it can just figure it out. By the same token, there can be basically *one* future AGI design, and if a copy wants to do industrial design, it can just figure it out!â€
> 
> Another place this comes up is robotics:
> 
> 1. *Economists:* â€œHumans will need to do R&D to invent good robotics algorithms.â€
> 2. *LLM-focused AGI person:* â€œFuture powerful AIs will need to do R&D to invent good robotics algorithms.â€
> 3. *Me:* â€œFuture powerful AI will *already be* a good robotics algorithm!â€
> 
> â€¦After all, if a human wants to use a new kind of teleoperated robot, nobody needs to do a big R&D project or breed a new subspecies of human. You just take an off-the-shelf bog-standard human brain, and if it wants to pilot a new teleoperated robot, it will just autonomously figure out how to do so, getting rapidly better within a few hours. By the same token, there can be *one* future AGI design, and it will be able to do that same thing.

[^1]: This part overlaps with my earlier post: [Applying traditional economic thinking to AGI: a trilemma](https://www.lesswrong.com/posts/TkWCKzWjcbfGzdNK5/applying-traditional-economic-thinking-to-agi-a-trilemma)