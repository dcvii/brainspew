---
title: "The dark factory is real, most developers are getting slower, and your org chart is the bottleneck (plus 5 prompts to get from level 2 to 5)"
source: "https://natesnewsletter.substack.com/p/the-5-level-framework-that-explains?publication_id=1373231&post_id=188188186&isFreemail=false&r=7br8e&triedRedirect=true"
author:
  - "[[By Nate]]"
published:
created: 2026-02-18
description:
tags:
  - "clippings"
---
---

---

Three engineers at StrongDM run a software factory where no human writes code and no human reviews code. The system takes specifications written in markdown, builds the software, tests it against behavioral scenarios, and produces shippable artifacts. The humans approve outcomes.

Meanwhile, 90% of Claude Code’s codebase was written by Claude Code itself. Internal teams at Anthropic describe operating at 70–90% AI-written code, with some workflows approaching nearly all implementation generated by Claude Code. Boris Cherny, who leads the project, hasn’t personally written code in over two months.

And at the same time, in one rigorous randomized controlled trial, experienced open-source developers working in codebases they already knew completed tasks 19% slower when using AI tools. Not faster. Slower. And they don’t know it. They predicted AI would make them 24% faster, and after the study they still believed it had made them 20% faster. They were wrong about the direction, let alone the magnitude.

The frontier teams aren’t just using better AI tools. They’ve rebuilt everything around a fundamentally different workflow — one where the bottleneck has moved from how fast you can write code to how precisely you can describe what should exist, and where the organizational structures designed for humans writing code have become friction rather than support. Most developers who call themselves “AI-native” are stuck reading every diff the AI produces. The teams running dark factories have moved past reading code entirely. The gap between those two realities is enormous, and closing it requires changes that go far beyond installing a better tool.

Here’s what’s inside:

- **How the dark factory actually works.** StrongDM’s three-person team, the spec-driven workflow, and the scenario architecture that prevents agents from gaming their own tests.
- **The machines building themselves.** 90% of Claude Code written by Claude Code, Codex 5.3 instrumental in creating itself, and what a closed feedback loop means.
- **The honest distance.** Why most developers are measurably slower with AI tools — and why they don’t know it.
- **The org chart problem.** What happens to sprint planning, code review, and engineering management when coordination becomes friction.
- **The legacy problem.** Why you can’t dark-factory your way through existing systems, and what the migration path actually looks like.
- **The talent reckoning.** Junior developer employment collapsing, the career ladder hollowing out, and the rising bar for what “good engineer” means.
- **The shape of the new org.** Tiny teams generating $100M+ in revenue and what the org chart looks like when agents do the implementation.
- **The tension that won’t resolve.** The frontier is further ahead and the middle is further behind than anyone wants to admit.

The dark factory doesn’t run on more engineers. It runs on engineers who can think clearly about what should exist, describe it precisely enough that machines can build it, and evaluate whether what got built actually serves the humans it was built for. That’s a fundamentally different job than the one most engineers were hired to do.

## LINK: Grab the prompts

The companion prompt kit for this article gives you the practical tools to start moving up the levels — not by installing a better AI coding assistant, but by developing the specification-writing discipline, the outcome-evaluation frameworks, and the organizational redesign thinking that separates teams operating at Level 5 from teams stuck at Level 2. The prompts help you document what your existing systems actually do (the unsexy prerequisite work that most teams skip), write specifications rigorous enough for autonomous agents to implement correctly, design behavioral scenarios that evaluate correctness rather than test passage, and identify which parts of your org chart are coordination structures built for a world where humans write code. This isn’t about getting AI to write more code faster. It’s about building the foundation that makes the dark factory possible — and that foundation is entirely human work.

## “Code must not be written by humans. Code must not be reviewed by humans.”

Those are the first two principles of a production software team called StrongDM’s Software Factory. Three engineers. No one writes code. No one reviews code. The system — a set of AI agents orchestrated by markdown spec files — takes a specification, builds the software, tests it against behavioral scenarios, and produces working software. The humans evaluate outcomes and approve what ships. The machines do everything in between.

Meanwhile, 90% of Claude Code’s codebase was written by Claude Code itself. Boris Cherny, who leads the project at Anthropic, hasn’t personally written code in over two months. Multiple teams inside Anthropic now describe the vast majority of their implementation code as AI-generated.

And meanwhile — at the same time, in the same industry, on the same planet — a rigorous 2025 randomized controlled trial by METR found that experienced open-source developers working in codebases they already knew took 19% longer to complete tasks when using AI tools than developers working without them. Not faster. Slower. The part that should unsettle you: those same developers predicted AI would make them 24% faster before the study began, and still believed it had made them 20% faster after the study ended. They were wrong about the direction, let alone the magnitude.

Three teams running lights-out software factories. The rest of the industry getting measurably slower while convinced they’re speeding up. The distance between those two realities is the most important gap in technology right now, and almost nobody is talking honestly about what it takes to cross it.

## The Five Levels

Dan Shapiro — CEO of Glowforge, veteran of multiple companies built on the boundary between software and physical products — published a framework in early 2026 that maps where the industry actually stands. He calls it the Five Levels of Vibe Coding, and the name is deliberately informal because the underlying reality is anything but.

Level 0: Spicy Autocomplete. GitHub Copilot in its original form — you type, it suggests the next line, you accept or reject. A faster tab key. The human still writes software; the AI just reduces keystrokes.

Level 1: Coding Intern. Hand the AI a discrete, well-scoped task — write this function, build this component, refactor this module — and review everything that comes back. The AI handles bounded tasks. Architecture, judgment, and integration stay with the human.

Level 2: Junior Developer. The AI handles multi-file changes. It can navigate a codebase, understand dependencies, build features that span modules. You’re reviewing more complex output, but you’re still reading every diff. Shapiro estimates 90% of developers who call themselves “AI-native” are operating at this level. They think they’re further along. They aren’t.

Level 3: Developer as Manager. This is where the relationship flips. You’re not writing code and having AI help. You’re directing AI and reviewing what it produces. Your day is diffs. You read, approve, reject, redirect. The model does the implementation. You do the judgment.

Almost everyone tops out here. “Most developers hit a ceiling at Level 3, and that ceiling has less to do with the tools than with the organizational and psychological difficulty of letting go of the code.”

Level 4: Developer as PM. You write a specification. You leave. You come back hours later and check whether the tests pass. You’re not reading diffs anymore. You’re evaluating outcomes. The code is a black box — you care whether it works, not how it’s written. This requires a level of trust in the system, and a quality of specification writing, that almost no one has developed yet.

Level 5: The Dark Factory. A black box that turns specs into software. No human writes code. No human reviews code. The factory runs autonomously: specification in, working software out. Shapiro’s assessment is blunt — a handful of small teams, fewer than five people each, are operating at this level. The rest of the industry is somewhere between Level 1 and Level 3, with most clustering at Level 2.

The framework matters because it gives us honest language for a conversation that’s been drowning in hype. When a vendor tells you their tool “writes code for you,” they mean Level 1. When a startup says they’re doing “agentic software development,” they usually mean Level 2 or 3. When StrongDM says code must not be written by humans, they mean Level 5 — and they actually operate there. The gap between the marketing language and the operating reality is enormous, and collapsing that gap requires changes that go far beyond installing a better AI tool.

## What Level 5 Actually Looks Like

StrongDM’s Software Factory is the most thoroughly documented example of Level 5 in production. Simon Willison — one of the most careful and credible observers in the developer tooling space — called it “the most ambitious form of AI-assisted software development I’ve seen yet.” The details are worth understanding, because they reveal what it actually takes to run a dark factory, and why it’s so hard to replicate.

The team is three people. Justin McCarthy, CTO of StrongDM. Jay Taylor. Navan Chauhan. They’ve been running the factory since July 2025, and the inflection point they identify is Claude 3.5 Sonnet rev 2, which shipped in October 2024. That’s when long-horizon agentic coding started compounding correctness rather than compounding errors — the model could sustain coherent work across sessions long enough that the output was reliable, not just impressive.

The factory runs on an open-source coding agent called Attractor. The core of the repo is a small set of markdown specification files that describe what the software should do. The orchestration tooling around them is real, but the specifications are the center of gravity. The agent reads them, writes the code, and tests it.

But here’s where it gets interesting — and where most people’s mental model breaks down. StrongDM doesn’t use traditional software tests. They use what they call scenarios. The distinction matters. Tests live inside the codebase. The AI agent can read them. Which means the agent can — intentionally or not — optimize for passing the tests rather than building correct software. It’s the same problem as teaching to the test in education: perfect scores, shallow understanding.

Scenarios live outside the codebase. They’re behavioral specifications that describe what the software should do from an external perspective, stored separately from the agent’s development context to prevent overfitting. They function as a holdout set — the same concept that machine learning uses to prevent overfitting. The agent builds the software. The scenarios evaluate whether the software actually works. The agent never sees the evaluation criteria. It can’t game them.

This is a genuinely new idea in software development, and it solves a problem that nobody was thinking about when all the code was written by humans. When humans write code, we don’t worry about the developer gaming their own test suite. When AI writes code, optimizing for test passage is the default behavior unless you deliberately architect around it. StrongDM architected around it.

The other piece is what they call the Digital Twin Universe — behavioral clones of every external service the software interacts with. A simulated Okta. A simulated Jira. Simulated Slack, Google Docs, Google Drive, Google Sheets. The AI agents develop against these digital twins, which means they can run full integration scenarios without touching real production systems, real APIs, or real data. It’s a complete simulated environment purpose-built for autonomous software development.

The output is real. CXDB — their AI Context Store — is 16,000 lines of Rust, 9,500 lines of Go, and 6,700 lines of TypeScript. Shipped. In production. Built by agents.

And then the metric that tells you how seriously they take it: “If you haven’t spent at least $1,000 on tokens today per human engineer, your software factory has room for improvement.” That’s not a joke. At $1,000 per engineer per day in token spend, the AI agents are running at a volume that makes the compute cost meaningful — and still cheaper than the humans they’re replacing.

## The Machines Building Themselves

The self-referential loop has arrived, and it’s stranger than the hype makes it sound.

Codex 5.3 is the first frontier AI model that was instrumental in creating itself. Not metaphorically. Earlier builds of Codex analyzed training logs, flagged failing tests, suggested fixes to training scripts, generated deployment recipes, and summarized evaluation anomalies. The model that shipped is a product of its own predecessors’ labor. Coverage of the release cited a 25% speed improvement and dramatically reduced token waste, improvements attributed in part to earlier builds identifying their own inefficiencies.

Claude Code is further along. Ninety percent of the code in Claude Code — the tool itself — was written by Claude Code. Boris Cherny, who leads the project, told interviewers he hasn’t personally written code in over two months. He’s not joking and he’s not exaggerating — his role has shifted entirely to specification, direction, and judgment. Multiple teams inside Anthropic now describe the vast majority of their implementation code as AI-generated. The humans architect. The machines implement.

The downstream numbers tell the same story. Cowork — Anthropic’s desktop automation product — was built in ten days by four engineers, with all code written by Claude Code. SemiAnalysis estimates that roughly 4% of public commits on GitHub are now authored by Claude Code, a share Anthropic expects to exceed 20% by end of 2026. The tool hit a billion-dollar run rate six months after launch.

This is not a future state. This is February 2026. The tools are building themselves, improving themselves, and producing the next generation of the tools that will build the next generation after that. The feedback loop is closed. The question isn’t whether this will happen. The question is how fast the loop accelerates — and what it means for the forty million people worldwide who currently build software for a living.

## The Honest Distance

The gap between what’s possible at the frontier and what’s happening in practice is wider than any vendor wants to admit.

That METR study — a randomized controlled trial, not a survey, not a case study — found that experienced open-source developers working in codebases they already knew completed their tasks 19% slower when using AI coding tools than developers working without AI assistance. The researchers controlled for task difficulty, developer experience, and tool familiarity. The result held. AI made experienced developers slower.

Why? Because the workflow disruption outweighed the generation speed. Developers spent time evaluating AI suggestions, correcting almost-right code, context-switching between their own mental model and the model’s output, and debugging subtle errors introduced by generated code that looked correct but wasn’t. In Stack Overflow’s 2025 developer survey, roughly 46% of developers say they don’t fully trust AI-generated code. The frustration shows up in a consistent pattern: suggestions that are close enough to look correct, wrong enough to break in production. These aren’t Luddites. They’re experienced engineers running into the same problem: the AI is fast but not reliable enough to trust without careful review, and the review eats the time savings.

This is the J-curve that adoption researchers keep identifying. When you bolt an AI coding assistant onto an existing workflow, productivity dips before it improves — sometimes for months. The dip happens because the tool changes the workflow, but the workflow hasn’t been redesigned around the tool. You’re running a new engine on old transmission. The gears grind. Most organizations are sitting in the bottom of that J-curve right now, and many of them are interpreting the dip as evidence that AI tools don’t work, rather than evidence that their workflows haven’t adapted.

I’ve watched this play out with teams I work with directly, and the pattern is remarkably consistent. GitHub Copilot is the clearest illustration. Twenty million users and the dominant market position among AI coding tools. GitHub’s own research showed substantially faster code completion on isolated tasks. But in production, the story is more complicated: larger pull requests, higher review costs, more security vulnerabilities introduced by generated code that developers accepted without fully understanding. One senior engineering leader put it sharply: “Copilot makes writing code cheaper, but it makes owning code more expensive.”

The organizations seeing 25 to 30 percent productivity gains aren’t the ones that installed Copilot and called it done. They’re the ones that redesigned their entire development workflow around AI capabilities — changed how they write specs, changed how they review code, changed what they expect from junior versus senior engineers, changed their CI/CD pipelines to catch the new category of errors that AI-generated code introduces. End-to-end process transformation, not tool adoption. And end-to-end process transformation is hard, expensive, politically contentious, and slow. Which is why most companies haven’t done it. Which is why most companies are stuck in the bottom of the J-curve. Which is why the gap between frontier teams and everyone else keeps widening.

## The Org Chart Problem

Most software organizations were designed to facilitate people building software. Every process, every ceremony, every role — they exist because humans building software in teams need coordination structures.

Stand-up meetings exist because developers working on the same codebase need to synchronize daily. Sprint planning exists because humans can only hold a certain number of tasks in working memory and need a regular cadence to re-prioritize. Code review, QA handoffs, Jira boards, CI/CD gates — all of them are responses to the same underlying reality: humans building software in teams make mistakes, lose context, and need visibility into each other’s work. The entire coordination layer of a modern engineering org exists because of human limitations.

And when the human is no longer the one writing the code, the structures don’t just become optional — they become friction.

What does sprint planning look like when the implementation happens in hours, not weeks? What does code review look like when no human wrote the code and no human can efficiently review a thousand-line diff that an AI produced in twenty minutes? The questions cascade from there — QA, Jira, release management — and they all point in the same direction.

StrongDM’s three-person team doesn’t have sprints. They don’t have standups. They don’t have a Jira board. They write specifications and evaluate outcomes. The entire coordination layer that constitutes the operating system of a modern software organization — the layer that most engineering managers spend 60% of their time maintaining — doesn’t exist. Not because they eliminated it as a cost-saving measure. Because it serves no purpose when the agents do the implementation.

This is the structural shift that’s harder to see than the technology shift, and it might matter more. The question isn’t “can AI write code” — clearly it can. The question is: what happens to the organizational structures that were built for a world where humans write code? What happens to the engineering manager whose primary value is coordination? What happens to the scrum master? The release manager? The technical program manager whose job is to make sure twelve teams ship their pieces on time?

Those roles don’t disappear overnight. But their center of gravity shifts. The engineering manager’s value moves from “coordinate the team building the feature” to “define the specification clearly enough that agents build the right feature.” The program manager’s value moves from “track dependencies between human teams” to “architect the pipeline of specifications that flow through the factory.” The skills that matter shift from coordination to articulation — from making sure people are rowing in the same direction to making sure the direction is described precisely enough that machines can execute on it.

And if you think that’s a trivial shift — I keep meeting engineering leaders who do — you’ve never tried to write a specification detailed enough for an AI agent to implement correctly without human intervention. It’s a different skill. It requires the kind of rigorous systems thinking that most organizations have never needed from most of their people, because the humans on the other end of the spec could fill in the gaps with judgment, context, and a Slack message asking “did you mean X or Y?”

The machines don’t ask clarifying questions. They build what you described. If what you described was ambiguous, you get ambiguous software. The bottleneck has moved from implementation speed to the quality of what goes into the machine — and that quality is a function of how deeply you understand the system, the user, and the problem. That kind of understanding has always been the scarcest resource in software engineering. The dark factory doesn’t reduce the demand for it. It makes the demand absolute.

## The Legacy Problem

Everything above assumes you’re building from scratch. Most of the software economy isn’t.

The vast majority of enterprise software is brownfield — existing systems, accumulated over years or decades, running in production, serving real users, carrying real revenue. CRUD applications that process business transactions. Monoliths that have grown organically through fifteen years of feature additions. CI/CD pipelines tuned to the quirks of a specific codebase and a specific team’s workflow. Configuration management that exists in the heads of the three people who’ve been at the company long enough to remember why that one environment variable is set to that one value.

You cannot dark-factory your way through a legacy system. The specification doesn’t exist. The documentation, if there is any, is wrong. The tests, if there are any, cover 30% of the code and the other 70% runs on institutional knowledge and tribal lore. The system is the specification — it’s the only complete description of what the software does, because no one ever wrote down the thousand implicit decisions that accumulated over a decade of patches, hotfixes, and “temporary” workarounds that became permanent.

This is the honest truth about the interstitial states between Level 2 and Level 5: for most organizations, the path doesn’t start with “deploy an agent that writes code.” It starts with “develop a specification for what your existing software actually does.” And that specification work — reverse-engineering the implicit knowledge embedded in a running system — is deeply human work. It requires the engineer who knows why the billing module has that one edge case for Canadian customers. It requires the architect who remembers which microservice was carved out of the monolith under duress during the 2021 outage. It requires the product person who can explain what the software actually does for users versus what the PRD says it does. Domain expertise, customer understanding, systems thinking — exactly the human capabilities that matter more in the dark factory era, not less.

The migration path looks something like this. First, you use AI at Level 2 or 3 to accelerate the work your developers are already doing — writing new features, fixing bugs, refactoring modules. This is where most organizations are now, and it’s where the J-curve productivity dip happens. Second, you start using AI to document what your system actually does — generating specifications from code, building scenario suites that capture existing behavior, creating the holdout sets that a future dark factory will need. Third, you redesign your CI/CD pipeline to handle AI-generated code at volume — different testing strategies, different review processes, different deployment gates. Fourth, you begin shifting new development to Level 4 or 5 patterns while maintaining the legacy system in parallel.

That path takes years, not months. Anyone telling you otherwise is selling something. The organizations that will get there fastest aren’t the ones with the best AI tools. They’re the ones with the best specifications, the deepest domain understanding, and the discipline to invest in the boring, unglamorous work of documenting what their systems actually do before they hand the keys to the machines.

## The Talent Reckoning

Early-career developer employment dropped roughly 7 to 10 percent within six quarters of widespread AI coding tool adoption, according to research out of Harvard. In the UK, graduate technology roles fell 46% in 2024, with a further 53% drop projected by 2026, according to the Institute of Student Employers. In the US, some datasets suggest junior developer job postings have declined by more than 60%.

The junior developer pipeline is collapsing. And the implications go far beyond the people who can’t find entry-level jobs.

The career ladder in software engineering has always worked like this: juniors learn by doing — they write simple features, fix small bugs, absorb the codebase through immersion. Seniors review their work, mentor them, catch their mistakes. Over five to seven years, juniors become seniors through accumulated experience. The system is an apprenticeship model wearing enterprise clothing.

AI breaks that model at the bottom. If AI handles the simple features and small bug fixes — the work that juniors learn on — where do juniors learn? If AI reviews code faster and more thoroughly than a senior engineer doing a PR review, where does the mentorship happen? The career ladder is getting hollowed out from below: seniors at the top, AI at the bottom, and a thinning middle where the learning used to happen.

The pipeline is breaking. And yet — we need excellent engineers more than we’ve ever needed them. Not fewer engineers. Better ones. The bar is rising, and it’s rising toward exactly the skills that have always been the hardest to develop and the hardest to hire for.

The junior of 2026 needs the systems-design understanding that was expected of a mid-level engineer in 2020. Not because the entry-level work got harder, but because the entry-level work got automated and the remaining work requires deeper judgment. You don’t need someone who can write a CRUD endpoint — the AI handles that in seconds. You need someone who can look at a system architecture and identify where it will break under load, where the security model has gaps, where the user experience falls apart at edge cases, and where the business logic encodes assumptions that are about to become wrong.

Systems thinking. Customer intuition. The ability to hold a whole product in your head and reason about how the pieces interact. The ability to write a specification clear enough that an autonomous agent can implement it correctly — which requires understanding the problem deeply enough to anticipate the questions the agent won’t know to ask. Those skills have always separated great engineers from adequate ones. The difference now is that adequate is no longer a viable career position, because adequate is what the models do.

Anthropic’s hiring has already shifted. They’re preferring generalists over specialists — people who can think across domains rather than people who are expert in one narrow technology stack. The logic is straightforward: when AI handles implementation, the human’s value is in understanding the problem space broadly enough to direct implementation correctly. A specialist who knows everything about Kubernetes but can’t reason about the product implications of an architectural decision is less valuable than a generalist who understands systems, users, and business constraints even if they can’t hand-configure a pod.

Some organizations are moving toward what amounts to a medical residency model for junior engineers — simulated environments where early-career developers learn by working alongside AI systems, reviewing AI output, and developing judgment about what’s correct and what’s subtly wrong. It’s not the same as learning by writing code from scratch. But it might be the right training for a world where the job is directing and evaluating AI output rather than producing code from a blank editor.

Gartner projects that 80% of software engineers will need to upskill in AI-assisted development tools by 2027. That number is probably conservative. The question isn’t whether the skills need to change. It’s whether the industry can develop the training infrastructure fast enough to keep up with the pace of capability change — and whether the organizations that depend on software can tolerate a period where the talent pipeline is being rebuilt while the old one is collapsing.

## The Shape of the New Org

Cursor — the AI-native code editor — hit $100 million in annual recurring revenue in twelve months with fewer than twenty employees, then reportedly blew past $500 million ARR by mid-2025 with a team that had grown to roughly forty people. Even at the reported headcount, that’s over $12 million in revenue per employee — multiples beyond what even high-performing SaaS companies generate per head. Midjourney reportedly generated around $500 million in revenue with between 107 and 163 people, depending on the estimate. Lovable hit $100 million in ARR in eight months with roughly 45 people. Bolt reached $20 million ARR in two months with fewer than twenty people.

The leanest AI-native startups are generating revenue per employee that dwarfs typical SaaS benchmarks by five to ten times or more. The exact multiples vary, but the pattern is consistent: tiny teams, enormous output. These aren’t outliers anymore. They’re the template.

What does an organization look like when a small team can build a product that generates $100 million a year? It doesn’t look like a traditional software company with an engineering team, a product team, a QA team, a DevOps team, and a program management office. It looks like a small group of people who are exceptionally good at understanding what users need, translating that understanding into clear specifications, and directing AI systems that handle the implementation.

The org chart flattens radically. The layers of coordination that exist to manage hundreds of engineers building a product become unnecessary when the engineering is done by agents. The middle management layer — the engineering managers, the tech leads, the scrum masters, the release managers — either evolves into something fundamentally different or ceases to exist. The people who remain are the ones whose judgment can’t be automated: the ones who know what to build, for whom, and why.

The restructuring is real, it’s in progress, and it will be painful for specific people in specific roles. The middle management layer, the junior developers whose entry-level work gets automated first, the QA engineers running manual test passes, the release managers whose value is coordination — those roles either transform or they contract. For the people in those roles, the abstract argument that “demand for software is higher than ever” is cold comfort when their particular job description no longer matches what the organization needs.

But the abstract argument is still true, and ignoring it would be as dishonest as ignoring the pain.

The part that gets lost in every conversation about AI and jobs is that we have never found a ceiling on the demand for software, and we have never found a ceiling on the demand for intelligence. Every time the cost of computing dropped — mainframes to PCs, PCs to cloud, cloud to serverless — the total amount of software the world produced didn’t stay flat. It exploded. New categories of software that were economically impossible at the old cost structure became viable, then ubiquitous, then essential. The cloud didn’t just make existing software cheaper to run. It created SaaS, mobile apps, streaming, real-time analytics, and a hundred other categories that couldn’t exist when you had to buy a rack of servers to ship a product.

The same dynamic applies now, at a scale that dwarfs every previous transition. Every company in every industry needs software. Most of them — the regional hospital, the mid-market manufacturer, the family logistics company — can’t afford to build what they need at current labor costs. A custom inventory system costs $500,000 and takes eighteen months. A patient portal integration costs $200,000. A supply chain dashboard that actually reflects the business costs six figures minimum. These companies make do with spreadsheets and manual workarounds because the software they need is economically out of reach.

Drop the cost of production by an order of magnitude and that unmet demand becomes addressable. Not theoretically. Practically. Teams of 20 to 45 people building $100M products aren’t just more efficient versions of traditional software companies — they’re serving markets that traditional software companies couldn’t afford to enter. The total addressable market for software at 2024 production costs is a fraction of the total addressable market for software at 2027 production costs. The denominator is moving.

This is not a comfortable-sounding rebuttal to the pain of specific jobs disappearing. It’s a structural observation about what happens when intelligence gets cheaper: the demand for it goes up, not down. We’ve watched this happen with compute, with storage, with bandwidth, with every resource that’s ever gotten dramatically cheaper. Demand has never saturated. The constraint has always moved to the next bottleneck — which, in this case, is the judgment to know what to build and for whom.

The people who thrive will be the ones who were always the hardest to replace: the ones who understand customers deeply, who think in systems, who can hold ambiguity and make decisions under uncertainty, who can articulate what needs to exist before it exists. The dark factory doesn’t replace those people. It amplifies them. It turns a great product thinker with five engineers into a great product thinker with unlimited engineering capacity. The constraint moves from “can we build it” to “should we build it” — and “should we build it” has always been the harder, more valuable question.

## The Tension

I won’t resolve this, because resolution would be dishonest.

The dark factory is real. It works. A small number of teams are producing software without humans writing or reviewing code, and the output is shippable, production-grade, and improving with every model generation. The tools are building themselves. The feedback loop is closed. This is not speculative.

And yet most companies are stuck at Level 2, getting measurably slower with AI tools they believe are making them faster, running organizational structures designed for a world where humans do the implementation, and sitting on legacy codebases that no agent can navigate without the institutional knowledge locked in the heads of people who’ve been there for a decade.

Both things are true at the same time. The frontier is further ahead than most people realize. The middle is further behind than the frontier teams want to admit. And the distance between them isn’t primarily a technology gap — it’s a specification gap, an organizational gap, a talent gap, and a willingness-to-change gap that no software update closes automatically.

The enterprises that will cross that distance aren’t the ones that buy the best AI coding tool. They’re the ones that do the hard, slow, unglamorous work of documenting what their systems actually do, rebuilding their org charts around judgment instead of coordination, investing in the kind of talent that understands systems and customers deeply enough to direct machines that can build anything — and honest enough with themselves to admit that the transition takes years, not quarters.

The dark factory doesn’t need more engineers. It needs better ones. And “better” means something different than it did two years ago. It means the people who can think clearly about what should exist, describe it precisely enough that machines can build it, and evaluate whether what got built actually serves the humans it was built for.

That has always been the hardest part of software engineering. We just used to let the implementation complexity hide how few people were actually good at it.

The machines stripped the camouflage. Now we’ll find out.