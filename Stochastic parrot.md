---
title: "Stochastic parrot"
source: "https://en.wikipedia.org/wiki/Stochastic_parrot"
author:
  - "[[By Contributors to Wikimedia projects]]"
published:
created: 2026-02-18
description:
tags:
  - "clippings"
---
---

---

Term used in machine learning

In [machine learning](https://en.wikipedia.org/wiki/Machine_learning "Machine learning"), the term **stochastic parrot** is a metaphor, introduced by [Emily M. Bender](https://en.wikipedia.org/wiki/Emily_M._Bender "Emily M. Bender") and colleagues in a 2021 paper, that frames [large language models](https://en.wikipedia.org/wiki/Large_language_model "Large language model") as systems that statistically mimic text without real understanding.<sup><a href="https://en.wikipedia.org/wiki/#cite_note-parrot-paper-1"><span>[</span>1<span>]</span></a></sup> <sup><a href="https://en.wikipedia.org/wiki/#cite_note-Zimmer-2"><span>[</span>2<span>]</span></a></sup> The term carries a negative connotation.<sup><a href="https://en.wikipedia.org/wiki/#cite_note-Zimmer-2"><span>[</span>2<span>]</span></a></sup>

## Origin and definition

\[[edit](https://en.wikipedia.org/w/index.php?title=Stochastic_parrot&action=edit&section=1 "Edit section: Origin and definition")\]

The term was first used in the paper "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ðŸ¦œ" by Bender, [Timnit Gebru](https://en.wikipedia.org/wiki/Timnit_Gebru "Timnit Gebru"), Angelina McMillan-Major, and [Margaret Mitchell](https://en.wikipedia.org/wiki/Margaret_Mitchell_\(scientist\) "Margaret Mitchell (scientist)") (using the pseudonym "Shmargaret Shmitchell").<sup><a href="https://en.wikipedia.org/wiki/#cite_note-parrot-paper-1"><span>[</span>1<span>]</span></a></sup> <sup><a href="https://en.wikipedia.org/wiki/#cite_note-Zimmer-2"><span>[</span>2<span>]</span></a></sup> They argued that [large language models](https://en.wikipedia.org/wiki/Large_language_model "Large language model") (LLMs) present dangers such as environmental and financial costs, inscrutability leading to unknown dangerous biases, and potential for deception, and that they can't understand the concepts underlying what they learn.<sup><a href="https://en.wikipedia.org/wiki/#cite_note-3"><span>[</span>3<span>]</span></a></sup>

The word "stochastic" â€“ from the ancient Greek "ÏƒÏ„Î¿Ï‡Î±ÏƒÏ„Î¹ÎºÏŒÏ‚" (*stokhastikos*, "based on guesswork")Â â€“ is a term from [probability theory](https://en.wikipedia.org/wiki/Probability_theory "Probability theory") meaning "randomly determined".<sup><a href="https://en.wikipedia.org/wiki/#cite_note-Zimmer-2"><span>[</span>2<span>]</span></a></sup> The word "parrot" refers to [parrots](https://en.wikipedia.org/wiki/Parrot "Parrot") ' ability to [mimic human speech](https://en.wikipedia.org/wiki/Parrot#Sound_imitation_and_speech "Parrot"), without understanding its meaning.<sup><a href="https://en.wikipedia.org/wiki/#cite_note-Zimmer-2"><span>[</span>2<span>]</span></a></sup>

In their paper, [Bender](https://en.wikipedia.org/wiki/Emily_M._Bender "Emily M. Bender") et al. state that LLMs are "stitching together sequences of linguistic forms... observed in its vast training data, according to probabilistic information about how they combine, but without any reference to meaning." Therefore, they are labeled to be mere "stochastic parrots".<sup><a href="https://en.wikipedia.org/wiki/#cite_note-parrot-paper-1"><span>[</span>1<span>]</span></a></sup> According to the machine learning professionals Lindholm, WahlstrÃ¶m, Lindsten, and SchÃ¶n, the analogy highlights two vital limitations:<sup><a href="https://en.wikipedia.org/wiki/#cite_note-FOOTNOTELindholmWahlstr%C3%B6mLindstenSch%C3%B6n2022322%E2%80%933-4"><span>[</span>4<span>]</span></a></sup> <sup><a href="https://en.wikipedia.org/wiki/#cite_note-Uddin2-5"><span>[</span>5<span>]</span></a></sup>

- LLMs are limited by the data they are trained by and are simply stochastically repeating contents of datasets.
- Because they are just making up outputs based on training data, LLMs do not understand if they are saying something incorrect or inappropriate.

Lindholm et al. noted that, with poor quality datasets and other limitations, a learning machine might produce results that are "dangerously wrong".<sup><a href="https://en.wikipedia.org/wiki/#cite_note-FOOTNOTELindholmWahlstr%C3%B6mLindstenSch%C3%B6n2022322%E2%80%933-4"><span>[</span>4<span>]</span></a></sup>

### Dismissal of Gebru by Google

\[[edit](https://en.wikipedia.org/w/index.php?title=Stochastic_parrot&action=edit&section=2 "Edit section: Dismissal of Gebru by Google")\]

Gebru was asked by Google to retract the paper or remove the names of Google employees from it. According to [Jeff Dean](https://en.wikipedia.org/wiki/Jeff_Dean "Jeff Dean"), the lead of Google AI at the time, the paper "didn't meet our bar for publication". In response, Gebru listed conditions to be met, stating that otherwise they could "work on a last date". Dean wrote that one of these conditions was for Google to disclose the reviewers of the paper and their specific feedback, which Google declined. Shortly after, she received an email saying that Google was "accepting her resignation". Her firing sparked a protest by Google employees, who believed the intent was to censor Gebru's criticism.<sup><a href="https://en.wikipedia.org/wiki/#cite_note-Lyons-2020-6"><span>[</span>6<span>]</span></a></sup>

The phrase has been used by AI skeptics to signify that LLMs lack understanding of the meaning of their outputs.

[Sam Altman](https://en.wikipedia.org/wiki/Sam_Altman "Sam Altman"), CEO of [OpenAI](https://en.wikipedia.org/wiki/OpenAI "OpenAI"), used the term shortly after the release of [ChatGPT](https://en.wikipedia.org/wiki/ChatGPT "ChatGPT"), when he tweeted "i am a stochastic parrot, and so r u".<sup><a href="https://en.wikipedia.org/wiki/#cite_note-Zimmer-2"><span>[</span>2<span>]</span></a></sup> The term was designated to be the 2023 AI-related Word of the Year by the [American Dialect Society](https://en.wikipedia.org/wiki/American_Dialect_Society "American Dialect Society").<sup><a href="https://en.wikipedia.org/wiki/#cite_note-7"><span>[</span>7<span>]</span></a></sup> <sup><a href="https://en.wikipedia.org/wiki/#cite_note-8"><span>[</span>8<span>]</span></a></sup>

Some LLMs, such as ChatGPT, have become capable of interacting with users in convincingly human-like conversations.<sup><a href="https://en.wikipedia.org/wiki/#cite_note-Arkoudas-2023-9"><span>[</span>9<span>]</span></a></sup> The development of these new systems has deepened the discussion of the extent to which LLMs understand or are simply "parroting".

### Subjective experience

\[[edit](https://en.wikipedia.org/w/index.php?title=Stochastic_parrot&action=edit&section=5 "Edit section: Subjective experience")\]

In the mind of a human being, words and language correspond to things one has experienced.<sup><a href="https://en.wikipedia.org/wiki/#cite_note-Fayyad-2023-10"><span>[</span>10<span>]</span></a></sup> For LLMs, words may correspond only to other words and patterns of usage fed into their training data.<sup><a href="https://en.wikipedia.org/wiki/#cite_note-Saba-2023-11"><span>[</span>11<span>]</span></a></sup> <sup><a href="https://en.wikipedia.org/wiki/#cite_note-Mitchell-2023-12"><span>[</span>12<span>]</span></a></sup> <sup><a href="https://en.wikipedia.org/wiki/#cite_note-parrot-paper-1"><span>[</span>1<span>]</span></a></sup> Proponents of the idea of stochastic parrots thus conclude that statements about LLMs are due to "the human tendency to attribute meaning to text",<sup><a href="https://en.wikipedia.org/wiki/#cite_note-parrot-paper-1"><span>[</span>1<span>]</span></a></sup> and claim this occurs despite the LLMs not actually understanding language.<sup><a href="https://en.wikipedia.org/wiki/#cite_note-Saba-2023-11"><span>[</span>11<span>]</span></a></sup> <sup><a href="https://en.wikipedia.org/wiki/#cite_note-parrot-paper-1"><span>[</span>1<span>]</span></a></sup>

[Kelsey Piper](https://en.wikipedia.org/wiki/Kelsey_Piper "Kelsey Piper") argued that the claim that LLMs are stochastic parrots or mere "next-token predictors" focuses on pre-training, ignoring that modern LLMs are also [fine-tuned](https://en.wikipedia.org/wiki/Fine-tuning_\(deep_learning\) "Fine-tuning (deep learning)") to follow instructions and to prefer accurate answers.<sup><a href="https://en.wikipedia.org/wiki/#cite_note-13"><span>[</span>13<span>]</span></a></sup>

### Hallucinations and mistakes

\[[edit](https://en.wikipedia.org/w/index.php?title=Stochastic_parrot&action=edit&section=7 "Edit section: Hallucinations and mistakes")\]

The tendency of LLMs to pass off false information as fact is held as support.<sup><a href="https://en.wikipedia.org/wiki/#cite_note-Fayyad-2023-10"><span>[</span>10<span>]</span></a></sup> Called [hallucinations](https://en.wikipedia.org/wiki/Hallucination_\(artificial_intelligence\) "Hallucination (artificial intelligence)") or confabulations, LLMs will occasionally synthesize information that matches some pattern.<sup><a href="https://en.wikipedia.org/wiki/#cite_note-Saba-2023-11"><span>[</span>11<span>]</span></a></sup> <sup><a href="https://en.wikipedia.org/wiki/#cite_note-Mitchell-2023-12"><span>[</span>12<span>]</span></a></sup> <sup><a href="https://en.wikipedia.org/wiki/#cite_note-Fayyad-2023-10"><span>[</span>10<span>]</span></a></sup> LLMs may fail to distinguish fact and fiction, which leads to the claim that they can't connect words to a comprehension of the world, as humans do.<sup><a href="https://en.wikipedia.org/wiki/#cite_note-Saba-2023-11"><span>[</span>11<span>]</span></a></sup> <sup><a href="https://en.wikipedia.org/wiki/#cite_note-Fayyad-2023-10"><span>[</span>10<span>]</span></a></sup> Furthermore, LLMs may fail to decipher complex or ambiguous grammar cases that rely on understanding the meaning of language.<sup><a href="https://en.wikipedia.org/wiki/#cite_note-Saba-2023-11"><span>[</span>11<span>]</span></a></sup> <sup><a href="https://en.wikipedia.org/wiki/#cite_note-Mitchell-2023-12"><span>[</span>12<span>]</span></a></sup> As an example, borrowing from Saba et al., is the prompt:<sup><a href="https://en.wikipedia.org/wiki/#cite_note-Saba-2023-11"><span>[</span>11<span>]</span></a></sup>

> The wet newspaper that fell down off the table is my favorite newspaper. But now that my favorite newspaper fired the editor I might not like reading it anymore. Can I replace 'my favorite newspaper' by 'the wet newspaper that fell down off the table' in the second sentence?

Some LLMs respond to this in the affirmative, not understanding that the meaning of "newspaper" is different in these two contexts; it is first an object and second an institution.<sup><a href="https://en.wikipedia.org/wiki/#cite_note-Saba-2023-11"><span>[</span>11<span>]</span></a></sup> Based on these failures, some AI professionals conclude they are no more than stochastic parrots.<sup><a href="https://en.wikipedia.org/wiki/#cite_note-Saba-2023-11"><span>[</span>11<span>]</span></a></sup> <sup><a href="https://en.wikipedia.org/wiki/#cite_note-Fayyad-2023-10"><span>[</span>10<span>]</span></a></sup> <sup><a href="https://en.wikipedia.org/wiki/#cite_note-parrot-paper-1"><span>[</span>1<span>]</span></a></sup>

### Benchmarks and experiments

\[[edit](https://en.wikipedia.org/w/index.php?title=Stochastic_parrot&action=edit&section=8 "Edit section: Benchmarks and experiments")\]

One argument against the hypothesis that LLMs are stochastic parrot is their results on [benchmarks](https://en.wikipedia.org/wiki/Benchmark_\(computing\) "Benchmark (computing)") for reasoning, common sense and language understanding. In 2023, some LLMs have shown good results on many language understanding tests, such as the Super General Language Understanding Evaluation (SuperGLUE).<sup><a href="https://en.wikipedia.org/wiki/#cite_note-Mitchell-2023-12"><span>[</span>12<span>]</span></a></sup> <sup><a href="https://en.wikipedia.org/wiki/#cite_note-14"><span>[</span>14<span>]</span></a></sup> GPT-4 scored in the >90th-percentile on the Uniform Bar Examination and achieved 93% accuracy on the MATH benchmark of high-school Olympiad problems, results that exceed rote pattern-matching expectations.<sup><a href="https://en.wikipedia.org/wiki/#cite_note-GPT4Report-15"><span>[</span>15<span>]</span></a></sup> Such tests, and the smoothness of many LLM responses, help as many as 51% of AI professionals believe they can truly understand language with enough data, according to a 2022 survey.<sup><a href="https://en.wikipedia.org/wiki/#cite_note-Mitchell-2023-12"><span>[</span>12<span>]</span></a></sup>

Leading AI researchers dispute the notion that LLMs merely "parrot" their training data.

- [Geoffrey Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton "Geoffrey Hinton"), a pioneering figure in neural networks, counters that the metaphor misunderstands the prerequisite for accurate language prediction. He argues that "to predict the next word accurately, you have to understand the sentence", a view he presented on *60 Minutes* in 2023.<sup><a href="https://en.wikipedia.org/wiki/#cite_note-HintonCBS-16"><span>[</span>16<span>]</span></a></sup> From this perspective, understanding is not an alternative to statistical prediction, but rather an emergent property required to perform it effectively at scale. Hinton also uses logical puzzles to demonstrate that LLMs actually understand language.<sup><a href="https://en.wikipedia.org/wiki/#cite_note-17"><span>[</span>17<span>]</span></a></sup>
- A 2024 *[Scientific American](https://en.wikipedia.org/wiki/Scientific_American "Scientific American")* investigation described a closed Berkeley workshop where state-of-the-art models solved novel tier-4 mathematics problems and produced coherent proofs, indicating reasoning abilities beyond memorization.<sup><a href="https://en.wikipedia.org/wiki/#cite_note-SciAmMath-18"><span>[</span>18<span>]</span></a></sup>
- The *GPT-4 Technical Report* showed human-level results on professional and academic exams (e.g., the [Uniform Bar Exam](https://en.wikipedia.org/wiki/Uniform_Bar_Exam "Uniform Bar Exam") and [USMLE](https://en.wikipedia.org/wiki/USMLE "USMLE")), challenging the "parrot" characterization.<sup><a href="https://en.wikipedia.org/wiki/#cite_note-GPT4Report-15"><span>[</span>15<span>]</span></a></sup>

Another line of evidence against the 'stochastic parrot' claim comes from [mechanistic interpretability](https://en.wikipedia.org/wiki/Mechanistic_interpretability "Mechanistic interpretability"), a research field dedicated to [reverse-engineering](https://en.wikipedia.org/wiki/Reverse_engineering "Reverse engineering") LLMs to understand their internal workings. Rather than only observing the model's input-output behavior, these techniques probe the model's internal activations, which can be used to determine if they contain structured representations of the world. The goal is to investigate whether LLMs are merely manipulating surface statistics or if they are building and using internal "world models" to process information.

One example is Othello-GPT, where a small [transformer](https://en.wikipedia.org/wiki/Transformer_\(deep_learning_architecture\) "Transformer (deep learning architecture)") was trained to predict legal [Othello](https://en.wikipedia.org/wiki/Reversi "Reversi") moves. It has been found that this model has an internal representation of the Othello board, and that modifying this representation changes the predicted legal Othello moves in the correct way. This supports the idea that LLMs have a "world model", and are not just doing superficial statistics.<sup><a href="https://en.wikipedia.org/wiki/#cite_note-19"><span>[</span>19<span>]</span></a></sup> <sup><a href="https://en.wikipedia.org/wiki/#cite_note-20"><span>[</span>20<span>]</span></a></sup>

In another example, a small transformer was trained on computer programs written in the programming language [Karel](https://en.wikipedia.org/wiki/Karel_\(programming_language\) "Karel (programming language)"). Similar to the Othello-GPT example, this model developed an internal representation of Karel program semantics. Modifying this representation results in appropriate changes to the output. Additionally, the model generates correct programs that are, on average, shorter than those in the training set.<sup><a href="https://en.wikipedia.org/wiki/#cite_note-21"><span>[</span>21<span>]</span></a></sup>

Researchers also studied " [grokking](https://en.wikipedia.org/wiki/Grokking_\(machine_learning\) "Grokking (machine learning)") ", a phenomenon where an AI model initially memorizes the training data outputs, and then, after further training, suddenly finds a solution that generalizes to unseen data.<sup><a href="https://en.wikipedia.org/wiki/#cite_note-22"><span>[</span>22<span>]</span></a></sup>

### Shortcut learning and benchmark flaws

\[[edit](https://en.wikipedia.org/w/index.php?title=Stochastic_parrot&action=edit&section=11 "Edit section: Shortcut learning and benchmark flaws")\]

A significant counterpoint in the debate is the well-documented phenomenon of "shortcut learning." <sup><a href="https://en.wikipedia.org/wiki/#cite_note-:0-23"><span>[</span>23<span>]</span></a></sup> Critics of claims for LLM understanding argue that high benchmark scores can be misleading.

When tests created to test people for language comprehension are used to test LLMs, they sometimes result in false positives caused by spurious correlations within text data.<sup><a href="https://en.wikipedia.org/wiki/#cite_note-24"><span>[</span>24<span>]</span></a></sup> Models have shown examples of shortcut learning, which is when a system makes unrelated correlations within data instead of using human-like understanding.<sup><a href="https://en.wikipedia.org/wiki/#cite_note-:0-23"><span>[</span>23<span>]</span></a></sup>

One such experiment conducted in 2019 tested Google's [BERT](https://en.wikipedia.org/wiki/BERT_\(language_model\) "BERT (language model)") LLM using the argument reasoning comprehension task. BERT was prompted to choose between 2 statements, and find the one most consistent with an argument. Below is an example of one of these prompts:<sup><a href="https://en.wikipedia.org/wiki/#cite_note-Mitchell-2023-12"><span>[</span>12<span>]</span></a></sup> <sup><a href="https://en.wikipedia.org/wiki/#cite_note-Niven-2019-25"><span>[</span>25<span>]</span></a></sup>

> Argument: Felons should be allowed to vote. A person who stole a car at 17 should not be barred from being a full citizen for life.  
> Statement A: Grand theft auto is a felony.  
> Statement B: Grand theft auto is not a felony.

Researchers found that specific words such as "not" hint the model towards the correct answer, allowing near-perfect scores when included but resulting in random selection when hint words were removed.<sup><a href="https://en.wikipedia.org/wiki/#cite_note-Mitchell-2023-12"><span>[</span>12<span>]</span></a></sup> <sup><a href="https://en.wikipedia.org/wiki/#cite_note-Niven-2019-25"><span>[</span>25<span>]</span></a></sup> This problem, and the known difficulties defining intelligence, causes some to argue all benchmarks that find understanding in LLMs are flawed, that they all allow shortcuts to fake understanding.